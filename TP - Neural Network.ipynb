{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construction d'un réseau de Neurones MLP\n",
    "## Structure des données\n",
    "On suppose que l'on a $n$ données différentes dans $\\mathbb{R}^p$. Ces données sont stockées dans une grande matrice de taille $(p,n)$ dont la $j$-ème colonne est un vecteur de taille $\\mathbb{R}^p$ qui représente la $j$-eme donnée.\n",
    "On note cette matrice $X_j[i]$ avec $1\\le i \\le p$ et $1\\le j \\le n$ tel que le vecteur $X_j$ est la $j$-eme donnée d'entrée.\n",
    "Ainsi l'exemple suivant représente 4 données dans $\\mathbb{R}^3$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  2  3  4]\n",
      " [ 5  6  7  8]\n",
      " [ 9 10 11 12]]\n"
     ]
    }
   ],
   "source": [
    "X =np.array([[1,2,3,4],[5,6,7,8],[9,10,11,12]])\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La première opération que l'on veut faire est, étant donné une matrice $A$ de taille $(q,p)$, de trouver la matrice $Y$ de taille $(q,n)$ telle que pour chaque donnée $j$, on ait $Y_j=AX_j$. Dans l'exemple suivant $q=2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A= [[1 2 3]\n",
      " [4 5 6]]\n",
      "Y= [[ 38  44  50  56]\n",
      " [ 83  98 113 128]]\n"
     ]
    }
   ],
   "source": [
    "A=np.array([[1,2,3],[4,5,6]])\n",
    "print('A=',A)\n",
    "Y= np.matmul(A,X)\n",
    "print ('Y=',Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous devez trouver pour $Y$\n",
    "\n",
    "`[[ 38  44  50  56]\n",
    " [ 83  98 113 128]]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant on veut ajouter à $Y$ un vecteur $b \\in \\mathbb{R}^q$ tel que pour chaque donnée $j$ on ait $Z_j=Y_j+b$. Pour cela on utilise la commande suivante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 1. 1. 1. 1.]\n",
      " [2. 2. 2. 2. 2.]]\n"
     ]
    }
   ],
   "source": [
    "b=np.array([1,2])\n",
    "print(np.outer(b,np.ones(5)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Vous devez comprendre la commande précédente et vous en servir pour calculer le vecteur $Z$ tel que $Z_j=AX_j+b$ dans l'exemple suivant :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 54.  62.  72.  81.]\n",
      " [ 86.  97. 116. 131.]]\n"
     ]
    }
   ],
   "source": [
    "X =np.array([[1,1,3,4],[5,6,7,8],[9,10,11,12]])\n",
    "A=np.array([[1,5,3],[4,5,6]])\n",
    "b=np.array([1,3])\n",
    "Z= np.matmul(A,X) + np.outer(b,np.ones(4))\n",
    "print(Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous devez trouver pour $Z$\n",
    "\n",
    "`[[  54.   62.   72.   81.]\n",
    " [  86.   97.  116.  131.]]`\n",
    " ## Structure d'un réseau de Neurones\n",
    " Un réseau de Neurone est une classe notée `Network` qui contient une liste de couches qui sont des objets de la classe `Layer`. On s'intéresse d'abord à construire les couches avant de construire le réseau. La structure globale est la suivante, on la remplira au fur et à mesure du TP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer() : # Une classe \n",
    "    def __init__(self,nb_entree,nb_sortie) :\n",
    "        self.n_entree = nb_entree\n",
    "        self.n_sortie= nb_sortie\n",
    "        self.A=np.random.randn(nb_sortie, nb_entree)\n",
    "        self.b=np.random.randn(nb_sortie)\n",
    "    def activation(self,u) :\n",
    "        return np.arctan(u)\n",
    "    def forward(self,X) : # calcul de activation(Ax+b)\n",
    "        Y = np.matmul(self.A, X) + np.outer(self.b, np.ones(X.shape[1]))\n",
    "        return self.activation(Y)\n",
    "    def deriv_activation(self,u):\n",
    "        pass\n",
    "    def backward(self,X,gx) :  # retropropagation du gradient sur la couche\n",
    "        pass\n",
    "\n",
    "class Network() : # Réseau de neurone qui est essentiellement une liste de Layers\n",
    "    def __init__(self,layers_dim) :\n",
    "        self.list_layers = []\n",
    "        for i in range(len(layers_dim) - 1):\n",
    "            self.list_layers.append(Layer(layers_dim[i], layers_dim[i +1]))\n",
    "    def forward(self,Z) : # calcul du passage dans chaque couche\n",
    "        X_list = []\n",
    "        X_list.append(np.copy(Z))\n",
    "        for layer in self.list_layers:            \n",
    "            Z = layer.forward(Z)\n",
    "            X_list.append(np.copy(Z))\n",
    "            \n",
    "        return X_list\n",
    "    def backward(self,X_list,gx) : # retropropagation globale du gradient\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calcul du forward du réseau de Neurone\n",
    "On va d'abord travailler sur la classe `Layer` et remplir la fonction `__init__()`. La fonction `__init__` prend en argument deux entiers `nb_entree` et `nb_sortie` (notés $p$ et $q$ dans ce qui suit) correspondant à respectivement à la taille des vecteurs d'entrée et la taille des vecteurs de sortie. Ces nombres doivent être stockés dans les variables internes `self.n_entree` et `self.n_sortie`. De plus la fonction `__init__` va tirer de manière aléatoire une matrice $A$ de taille $(q,p)$ (stockée dans `self.A`) et un vecteur $b$ (stocké dans `self.b`) de taille $q$. On utilisera un tirage selon une normale $(0,1)$ avec la fonction `random.randn` pour cela."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "2\n",
      "[[ 1.3315865   0.71527897 -1.54540029]\n",
      " [-0.00838385  0.62133597 -0.72008556]]\n",
      "[0.26551159 0.10854853]\n"
     ]
    }
   ],
   "source": [
    "# TEST DU RESEAU DE NEURONE\n",
    "np.random.seed(10)\n",
    "L=Layer(3,2)\n",
    "print(L.n_entree)\n",
    "print(L.n_sortie)\n",
    "print(L.A)\n",
    "print(L.b)\n",
    "# Vous devez trouver\n",
    "#3\n",
    "#2\n",
    "#A=[[ 1.3315865   0.71527897 -1.54540029]\n",
    "#[-0.00838385  0.62133597 -0.72008556]]\n",
    "#b=[ 0.26551159  0.10854853 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On remplit maintenant la fonction `forward` de la classe `Layer`. Etant donné une matrice $X$ de taille $(p,n)$ (où $n$ représente le nombre de données), la fonction `forward` calcule $Y$ de taille $(q,n)$ telle que $Y_j=AX_j+b$ pour tout $j$ entre $1$ et $n$ (on rappelle que $n$ est donné par `X.shape[1]`). On utilisera la section précédente pour faire ce calcul, on fera notamment attention à ne pas utiliser le vecteur $b$ directement dans la somme. Une fois que $Y$ est calculé, la fonction forward doit rendre `self.activation(Y)` (qui est ici déjà codée est qui est une arctangente)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.45681202 -1.44993537 -1.44218059 -1.43336915]\n",
      " [-1.2743528  -1.28322907 -1.29160274 -1.29951425]]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(10)\n",
    "L = Layer(3,2)\n",
    "X =np.array([[1,2,3,4],[5,6,7,8],[9,10,11,12]])\n",
    "print(L.forward(X))\n",
    "# Vous devez trouver\n",
    "#[[-1.45681202 -1.44993537 -1.44218059 -1.43336915]\n",
    "# [-1.2743528  -1.28322907 -1.29160274 -1.29951425]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Nous allons maintenant nous intéresser à la construction du réseau de Neurone. La classe Network prend en argument d'entrée `layers_dim` qui est une liste d'entier et qui correspond à la liste des dimensions des différentes couches. Si par exemple `layers_dim` vaut `[3,5,7,6]` alors le réseau de Neurone va prendre en argument des vecteurs de taille `layers_dim[0]=3` et va rendre au final des vecteurs de taille `layers_dim[-1]=6`. Au final le réseau de neurones sera composé de `3` couches, une de taille `(3,5)`, puis une de taille `(5,7)` puis une de taille `(7,6)`. Modifiez la fonction `__init__` de Network pour que le réseau construire ces couches et stocke les couches au fur et à mesure dans la liste `self.list_layers`. Puis lancez le code de vérification suivant :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "la couche 0 prend des vecteurs de taille 1 et sort des vecteurs de taille 5\n",
      "la couche 1 prend des vecteurs de taille 5 et sort des vecteurs de taille 2\n",
      "la couche 2 prend des vecteurs de taille 2 et sort des vecteurs de taille 6\n"
     ]
    }
   ],
   "source": [
    "N=Network([1,5,2,6])\n",
    "for i,layer in enumerate(N.list_layers) :\n",
    "    print('la couche '+str(i)+ ' prend des vecteurs de taille ' + str(layer.n_entree)+' et sort des vecteurs de taille '+ str(layer.n_sortie))   \n",
    "# Vous devez trouver :\n",
    "#la couche 0 prend des vecteurs de taille 1 et sort des vecteurs de taille 5\n",
    "#la couche 1 prend des vecteurs de taille 5 et sort des vecteurs de taille 2\n",
    "#la couche 2 prend des vecteurs de taille 2 et sort des vecteurs de taille 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalement on va programmer le réseau de Neurone en entier !! Il s'agit de la fonction forward de la classe Network, c'est très simple, on prend en argument une matrice `Z` et on la fait passer successivement dans toutes les couches du réseau. On pensera cependant à copier à chaque passage de couche le résultat dans une liste notée `X_list`, on en aura besoin pour plus tard. Attention aussi quand vous copiez un vecteur , utilisez la fonction `np.copy` sinon le vecteur n'est pas vraiment copié. On pensera aussi à copier le vecteur `Z` et à le mettre en tout début de `X_list`. Ainsi `X_list` est une suite de matrices dont la première dimension est exactement donnés par la variable `layers_dim`. La fonction `forward` devra rendre `X_list`. Le résultat du réseau de neurone est exactement donné par `X_list[-1]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 100)\n",
      "(5, 100)\n",
      "(6, 100)\n",
      "(7, 100)\n",
      "28.28071650303861\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(10)\n",
    "N=Network([3,5,6,7])\n",
    "X=np.arange(300).reshape(3,100)\n",
    "X_list=N.forward(X)\n",
    "for X in X_list :\n",
    "    print(X.shape)\n",
    "# Vous devez trouver\n",
    "#(3, 100)\n",
    "#(5, 100)\n",
    "#(6, 100)\n",
    "#(7, 100)\n",
    "print(np.linalg.norm(X_list[-1]))\n",
    "# vous devez trouver\n",
    "# 28.280716503"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calcul du gradient du réseau de Neurone\n",
    "On va calculer maintenant calculer la rétropropagation du gradient du réseau de neurone. Un conseil, commencez par copier votre classe ici pour ne plus toucher à la classe de la section précédente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer() : # Une classe \n",
    "    def __init__(self,nb_entree,nb_sortie) :\n",
    "        self.n_entree = nb_entree\n",
    "        self.n_sortie= nb_sortie\n",
    "        self.A=np.random.randn(nb_sortie, nb_entree)\n",
    "        self.b=np.random.randn(nb_sortie)\n",
    "    def activation(self,u) :\n",
    "        return np.arctan(u)\n",
    "    def forward(self,X) : # calcul de activation(Ax+b)\n",
    "        Y = np.matmul(self.A, X) + np.outer(self.b, np.ones(X.shape[1]))\n",
    "        return self.activation(Y)\n",
    "    def deriv_activation(self,u):\n",
    "        return 1 / (1 + (u)**2)\n",
    "    def backward(self,X,gx) :  # retropropagation du gradient sur la couche\n",
    "        temp1 = np.matmul(self.A, X)\n",
    "        temp2 = self.deriv_activation(temp1 + np.outer(self.b, np.ones(X.shape[1])))\n",
    "        M = np.multiply(temp2, gx)  \n",
    "        ge = np.matmul(np.matrix.transpose(self.A), M)\n",
    "        ga = np.matmul(M,np.matrix.transpose(X))\n",
    "        gb = np.zeros(self.n_sortie)\n",
    "        for i in range(self.n_sortie):\n",
    "            gb[i] = np.sum(M[i])\n",
    "        return  ge, ga, gb\n",
    "\n",
    "class Network() : # Réseau de neurone qui est essentiellement une liste de Layers\n",
    "    def __init__(self,layers_dim) :\n",
    "        self.list_layers = []\n",
    "        for i in range(len(layers_dim) - 1):\n",
    "            self.list_layers.append(Layer(layers_dim[i], layers_dim[i +1]))\n",
    "    def forward(self,Z) : # calcul du passage dans chaque couche\n",
    "        X_list = []\n",
    "        X_list.append(np.copy(Z))\n",
    "        for layer in self.list_layers:            \n",
    "            Z = layer.forward(Z)\n",
    "            X_list.append(np.copy(Z))\n",
    "            \n",
    "        return X_list\n",
    "    def backward(self,X_list,gx) : # retropropagation globale du gradient\n",
    "        list_grad = []\n",
    "        \n",
    "        for (layer, i) in zip(reversed(self.list_layers), reversed(range(len(X_list)))):\n",
    "            (gx, ga, gb) = layer.backward(X_list[i-1], gx)\n",
    "            list_grad.append((ga, gb))\n",
    "        return list(reversed(list_grad))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tout d'abord on s'occupe de la classe Layer, et on remplit la fonction `deriv_activation`. C'est simple, si on note $\\phi$, la fonction `activation`, alors on met la dérivée $\\phi'$ dans la fonction `deriv_activation`. A vous de jouer..\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.51048832 1.00075586 1.19839788]\n",
      "[0.76126675 0.29123952 0.13238721]\n"
     ]
    }
   ],
   "source": [
    "l=Layer(10,5)\n",
    "X=np.arange(3)+0.56\n",
    "print(l.activation(X))\n",
    "print(l.deriv_activation(X))\n",
    "# Vous devez trouver\n",
    "#[ 0.51048832  1.00075586  1.19839788]\n",
    "#[ 0.76126675  0.29123952  0.13238721]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C'était simple... Maintenant il faut coder la rétropropagation pour une couche. On rappelle que si la couche est de taille $(p,q)$ avec $n$ données alors l'algorithme prend en entrée `gx` un vecteur de taille $(q,n)$, calcule le gradient par rapport à $A$ et $b$ (qui sont donc des vecteurs de taille $(q,p)$ et $q$ respectivement) et rend un vecteur de taille $(p,n)$ qui sert à rétropropager le gradient aux couches précédentes. Les formules de calcul sont :\n",
    "\n",
    "$$ M=\\phi'(AX+b)*g_x $$\n",
    "$$g_e = A^T M$$\n",
    "$$g_A = M X^T$$\n",
    "$$g_b[i] = \\sum_j M_{ij}$$\n",
    "\n",
    "Ici $g_x$ est le vecteur de taille $(q,n)$ qui est donné en variable, $X$ est de taille $(p,n)$ et sont les données que la couche vient de traiter dans le calcul du forward (ces données sont stockées dans `X_list`), $M$ est un vecteur intermédiaire de taille $(q,n)$ (le produit donné dans la formule est donc entendu comme un produit terme à terme), ensuite $g_e$,$g_A$ et $g_b$ sont les trois gradients calculés (et rendus par l'algorithme). $g_e$ est le gradient utilisé dans la couche du dessous, $g_A$ et $g_b$ sont les gradients par rapport aux variables $self.A$ et $self.b$. Implémentez la fonction backward de Layer qui vous fait ces calculs et vous rend le triplet $g_e,g_A,g_b$ dans cet ordre.\n",
    "Testez votre code ci-dessous :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3, 10)\n",
      "(2, 3)\n",
      "(2,)\n",
      "[[ 0.37935363  0.59948105  1.00846542  1.8798579   4.14461384 11.62441892\n",
      "  24.25226532 11.42130188  4.7706869   2.53182615]\n",
      " [-0.05832252 -0.09247264 -0.15557944 -0.28972037 -0.63803699 -1.78809274\n",
      "  -3.72959337 -1.75699167 -0.73455346 -0.39037505]\n",
      " [-0.05831843 -0.08856387 -0.14876103 -0.28071541 -0.62737057 -1.7761148\n",
      "  -3.71655741 -1.74340955 -0.72054256 -0.37603143]]\n",
      "[[7.09467901e-01 1.93955318e+00 3.16963847e+00]\n",
      " [2.42292537e+02 6.52994735e+02 1.06369693e+03]]\n",
      "[ 0.12300853 41.07021978]\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "l=Layer(3,2)\n",
    "X=np.arange(30).reshape(3,10)\n",
    "GX=np.arange(20).reshape(2,10)\n",
    "(un,deux,trois)=l.backward(X,GX)\n",
    "for c in (un,deux,trois) :\n",
    "    print(c.shape)\n",
    "    # Vous devez trouver\n",
    "    #(3,10)\n",
    "    #(2,3)\n",
    "    #(2,)\n",
    "print(un)\n",
    "print(deux)\n",
    "print(trois)\n",
    "#Vous devez trouver\n",
    "#[[  0.37935363   0.59948105   1.00846542   1.8798579    4.14461384\n",
    "#   11.62441892  24.25226532  11.42130188   4.7706869    2.53182615]\n",
    "# [ -0.05832252  -0.09247264  -0.15557944  -0.28972037  -0.63803699\n",
    "#   -1.78809274  -3.72959337  -1.75699167  -0.73455346  -0.39037505]\n",
    "# [ -0.05831843  -0.08856387  -0.14876103  -0.28071541  -0.62737057\n",
    "#   -1.7761148   -3.71655741  -1.74340955  -0.72054256  -0.37603143]]\n",
    "#[[  7.09467901e-01   1.93955318e+00   3.16963847e+00]\n",
    "# [  2.42292537e+02   6.52994735e+02   1.06369693e+03]]\n",
    "#[  0.12300853  41.07021978]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le plus dur est fait... Il reste à coder le backward du réseau en entier. Pour cela on code maintenant la fonction backward de la classe Network. Le principe est simple, on prend en entrée un vecteur de taille $(q,n)$ où $q$ est la taille de la dernière couche et on calcule le gradient pour chaque couche en parcourant les couches en ordre inverse et en prenant pour la couche $s$ le gradient donné par la variable $g_e$ de la couche $s+1$...  On stocke la suite de variables $g_A$ et $g_b$ dans une liste que l'on rend. Si on appelle `list_grad` cette liste de gradient, ce doit être une liste de la taille du nombre de couches dont chaque élément contient un couple qui correspond au calcul de $(g_A,g_b)$ pour la couche en cours. Comme la liste 'list_grad' est construite à l'envers, on prendra bien soin de l'inverser avant de rendre le résultat.\n",
    "On aura besoin des fonctions suivantes sur les listes :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# parcours classique d'une liste\n",
      "alice\n",
      "bob\n",
      "charles\n",
      "# fonction zip qui permet de parcourir 2 listes en même temps\n",
      "alice toto\n",
      "bob titi\n",
      "charles tata\n",
      "# fonction reversed qui permet de parcourir une liste à l'envers\n",
      "charles\n",
      "bob\n",
      "alice\n",
      "# fonction qui permet d'inverser une liste\n",
      "['charles', 'bob', 'alice']\n"
     ]
    }
   ],
   "source": [
    "a=['alice','bob','charles']\n",
    "b=['toto','titi','tata']\n",
    "print(\"# parcours classique d'une liste\")\n",
    "for e in a :\n",
    "    print(e)\n",
    "print(\"# fonction zip qui permet de parcourir 2 listes en même temps\")\n",
    "for (e,f) in zip(a,b) :\n",
    "    print(e,f)\n",
    "print(\"# fonction reversed qui permet de parcourir une liste à l'envers\")\n",
    "for e in reversed(a) :\n",
    "    print(e) \n",
    "print(\"# fonction qui permet d'inverser une liste\")\n",
    "c=list(reversed(a))\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 3) (5,)\n",
      "(6, 5) (6,)\n",
      "(7, 6) (7,)\n",
      "########################\n",
      "245191.63608985924 1020.2244124421866\n",
      "19866.56223131648 6160.324519773959\n",
      "95230.47554231588 35836.81291508271\n"
     ]
    }
   ],
   "source": [
    "# Vous pouvez tester votre réseau de Neurone avec le code ci-dessous\n",
    "np.random.seed(42)\n",
    "N=Network([3,5,6,7])\n",
    "X0=np.arange(300).reshape(3,100)\n",
    "gX=np.arange(700).reshape(7,100)\n",
    "X_list=N.forward(X0)\n",
    "l=N.backward(X_list,gX)\n",
    "for (g_a,g_b) in l :\n",
    "    print (g_a.shape,g_b.shape)\n",
    "#Vous devez trouver\n",
    "#((5, 3), (5,))\n",
    "#((6, 5), (6,))\n",
    "#((7, 6), (7,))\n",
    "print(\"########################\")\n",
    "\n",
    "for (g_a,g_b) in l :\n",
    "    \n",
    "    print (np.linalg.norm(g_a),np.linalg.norm(g_b))\n",
    "#Vous devez trouver\n",
    "#(245191.63608985924, 1020.2244124421867)\n",
    "#(19866.562231316475, 6160.3245197739589)\n",
    "#(95230.475542315864, 35836.812915082708)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interfacage\n",
    "Ouf, votre reseau de neurones est terminé et opérationnel... On va maintenant ajouter des fonctions d'interfacages avec les autres programmes. Les autres programmes ne voient pas les variables du réseau de neurones comme une liste de matrice et de vecteurs mais comme un grand vecteur, il faut des fonctions pour faire l'interface, elles sont ci-dessous :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get(N) :\n",
    "    # prend un réseau de neurone et récupère les variables A et b de chaque couche dans un grand vecteur\n",
    "    n=0\n",
    "    #calcul de la taille du vecteur de sortie\n",
    "    for l in N.list_layers :\n",
    "        n+=(l.n_entree+1)*l.n_sortie\n",
    "    result=np.zeros(n) # on construit un vecteur de la bonne taille et on rend le réseau de Neurone\n",
    "    ind=0 # indice auquel on commence à construire le tableau\n",
    "    for l in N.list_layers :\n",
    "        result[ind:ind+l.n_entree*l.n_sortie]=l.A.flatten()\n",
    "        ind+=l.n_entree*l.n_sortie\n",
    "        result[ind:ind+l.n_sortie]=l.b.flatten()\n",
    "        ind+=l.n_sortie   \n",
    "    return result\n",
    "def set(N,C) :\n",
    "    # prend un reseau de neurone et met les variables A et b aux valeurs correspondantes de C\n",
    "    ind=0\n",
    "    for l in N.list_layers :\n",
    "        l.A=C[ind:ind+l.n_entree*l.n_sortie].reshape(l.n_sortie,l.n_entree)\n",
    "        ind+=l.n_entree*l.n_sortie\n",
    "        l.b[:]=C[ind:ind+l.n_sortie]\n",
    "        ind+=l.n_sortie\n",
    "def recup_grad(N,list_grad) :\n",
    "    # prend un réseau de neurone et la liste des gradients et la met dans un grand vecteur\n",
    "    n=0\n",
    "    #calcul de la taille du vecteur de sortie\n",
    "    for l in N.list_layers :\n",
    "        n+=(l.n_entree+1)*l.n_sortie\n",
    "    result=np.zeros(n) # on construit un vecteur de la bonne taille et on rend le réseau de Neurone\n",
    "    ind=0 # indice auquel on commence à construire le tableau\n",
    "    for l,(gA,gB) in zip(N.list_layers,list_grad) :\n",
    "        result[ind:ind+l.n_entree*l.n_sortie]=gA.flatten()\n",
    "        ind+=l.n_entree*l.n_sortie\n",
    "        result[ind:ind+l.n_sortie]=gB.flatten()\n",
    "        ind+=l.n_sortie   \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 0.0\n",
      "0.0\n",
      "(6, 2)\n",
      "(7, 6)\n",
      "(105,)\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(42)\n",
    "# On crée un réseau de neurone et on le lance sur des données\n",
    "N=Network([3,5,6,7])\n",
    "X=np.arange(6).reshape(3,2)\n",
    "X_list=N.forward(X)\n",
    "Y=np.copy(X_list[-1])\n",
    "\n",
    "\n",
    "# On récupère les paramètres et on les met à 0\n",
    "C=get(N)\n",
    "set(N,np.zeros(C.shape[0]))\n",
    "X_list=N.forward(X)\n",
    "Z=np.copy(X_list[-1])\n",
    "D=get(N)\n",
    "print(np.linalg.norm(D), np.linalg.norm(Z)) # on vérifie que tout est nul\n",
    "\n",
    "#on met les anciens paramètres et on vérifie que on obtient le bon résultat\n",
    "set(N,C)\n",
    "X_list=N.forward(X)\n",
    "Z=np.copy(X_list[-1])\n",
    "print(np.linalg.norm(Z-Y))\n",
    "\n",
    "# Test de la fonction recup_grad\n",
    "gX=np.arange(14).reshape(7,2)\n",
    "grad=recup_grad(N,N.backward(X_list,gX))\n",
    "print(grad.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si vous avez le temps et l'énergie vous pouvez incorporer comme il vous semble les fonctions d'interfacages directement dans la classe Network. Pensez à ne pas modifier votre classe actuelle mais bien à travailler sur une copie dans l'espace ci-dessous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
