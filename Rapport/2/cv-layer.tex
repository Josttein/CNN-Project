The convolutional layer is in many ways an extension of the regular fully-connected layer.
One of the issues with regular fully-connected layers is that these kinds of layers only accept input that is in the form of a vector.
This means that for applications where it is not natural for the input to be in a vector format, say image recognition, the input first has to be translated to a vector format.
Usually this results in a loss of information contained in the input.
In the typical case of image recongition, the input is in the form of one or more arrays of two dimensions.
For a multilayer percepetron to treat this input, the images has to be \quote{flattend} into a vector of one dimention before it can be passed on to the network.
This procedure eliminates some of the pixel relations in the image.
To deduce this, consider the process of reconstructing a flattend image.
If the image's dimensions prior to being flattend is not known, it is impossible, without the aid of pattern recognition, to reconstruct the image and be sure the reconstruction is equal to the original image.

To fix this problem, we can use a \quote{simple} solution.
Instead of having the neuron contain a vector of weights, let it have an array of weights.
These weight arrays are often called the neurons {\em kernel} or {\em filter}.
If we change the neuron's vector of weights into an array, we also need to change the operation that is used to combine the weights with the input (which in a neuron of a multilayer percepetron is the dot product).
There are two things to consider here.
The purpose of the weights is to look for features or {\em patterns} in the input---by emphasizing or deemphasizing certain aspects---and the operation must reflect this purpose of the weights.
Furthermore, the result of the operation should be a single number which, in a sense, representes the neurons \quote{initial} confidence that the feature it is looking for is present in the input.
The operation which does both of these things is the {\em Hadamard product}.
The Hadamard product can be seen as an extension of the dot product for two dimentional arrays.
We can express it as:
\startformula
{\rm Hp}(A,B) = \sum_{x} \sum_{y} a(x,y) b(x,y)
\stopformula
It combines two arrays---of the same dimensions---by multiplying corresponding entries together and summing the results.
Which is precisly what the dot product does with two vectors.

Another thing we need to take into account is the dimentions of our new weight array.
If we were to proceed analogically to how a multilayer percepetron works, the array should have the same size as the input to the neuron.
\quote{Connecting} each value in the input to an individual weight in the neuron.
But this approach means that each neuron, in principle, looks for a single feature in the entire input at once.
A more refined approach, is to let the neuron's weight array be smaller in size than its input.
Instead of applying the weight array (using the Hadamard product) on the entire input at once, we apply it (still using the Hadamard product) to portions of the input seperately.
Intuitivly, this means that the weight arrays are \quote{scanned} across the entire input image.
Figure~\in[conv-operation] illustrates this idea of the weight array scanning the input matrix to produce the output matrix.

A weight array that is smaller than the input matrix allows the neurons to be trained to look for a single small feature, such as a \quote{sharp edge} or a \quote{round corner}.
Which in can look for in multiple areas of the input.
As figure~\in[conv-operation] clearly shows, the output of the neurons is no longer be a single number representing how \quote{initialy} confident the neuron is that a particular feature is present in the input {\em\bf as a whole}.
Rather, the result becomes a {\em feature map}.
Another two dimentional array which representes how \quote{initialy} confident the neuron is that a particular feature is present in {\em\bf specific locations} of the input.

\startplacefigure[
    reference=conv-operation,
    title={The basic forward operation of a convolutional layer~\cite[escontrela_2018].},
    location=bottom,
]
\startcombination[3*3]
{\externalfigure[./Images/conv-0.jpg][width=.3\textwidth]}{}
{\externalfigure[./Images/conv-1.jpg][width=.3\textwidth]}{}
{\externalfigure[./Images/conv-2.jpg][width=.3\textwidth]}{}
{\externalfigure[./Images/conv-3.jpg][width=.3\textwidth]}{}
{\externalfigure[./Images/conv-4.jpg][width=.3\textwidth]}{}
{\externalfigure[./Images/conv-5.jpg][width=.3\textwidth]}{}
{\externalfigure[./Images/conv-6.jpg][width=.3\textwidth]}{}
{\externalfigure[./Images/conv-7.jpg][width=.3\textwidth]}{}
{\externalfigure[./Images/conv-8.jpg][width=.3\textwidth]}{}
\stopcombination
\stopplacefigure

Of course each neuron must be able to handle more than one input data stream each with its own input data matrix.
A typical example of a case where multiple streams are needed is the treatment of RGB images.
An RGB image consists of three arrays of pixel values (numbers) that desribe how red, green and blue an image is in each pixel.
Such an input would be split into three channels before being passed to a convolutional layer.
The neurons handle multiple data streams by simple adding together the feature maps produces by each data stream.
Figure~\in[cv-network] gives an overview of a convolutional layer.
The channels are drawn as pipes to emhpasize that they contain matrix data rather than scalar values.

\startplacefigure[
    reference=cv-network,
    location=bottom,
    title={A diagram of a convolutional layer},
]
\externalfigure[Images/cv-network.pdf][width=.7\textwidth]
\stopplacefigure

\startsubsubsection[title=Strides and zero padding]
There are two other aspects of convolutional layers that we have yet to discuss: strides and zero padding.
The stride is the number of rows/columns that the kernel moves over the input matrix between each scanning.
Figure~\in[conv-operation] shows a scenario where the stride is equal to one.

What purpose does altering the stride serve?
Increasing the stride reduces the size of the resulting feature map since it is computed at fewer areas.
Which has the net effect that the {\em computational complexity} of the neuron is reduced.
However, it also means that the neuron studies its input less which can result in vital information in the input being overlooked.
This is not a big problem as long as the features of the input is judged to be larger than the kernel that the neuron uses.
So long as this is the case, increasing the stride of a neuron can significantly reduce its computational complexity without resulting in it no longer surving its purpose.

Zero padding in one way serve the opposite purpose of increasing the stride.
Zero padding refers to the practice of \quote{padding} the input matrix with zeros around it.
This allows the neuron to retain the dimentions of the original input image in its feature map, if this is desired, without much increased computational complexity.
\stopsubsubsection

\startsubsubject[title=The equations]
The equations we are going to look at here does not use an altered stride nor do they take into account possible zero padding of the input.
To take the stride into account it suffices to multiply the red indices of the previous layer's activation with the stride value (e.g.\ $\color[red]{x}s$).

Let $l$ be the index of a convolutional layer.
Let $k_x$ and $k_y$ be the dimentions of the weight matricies in this layer.
The dimentions between layer $l$ and layer $l-1$ are given by the following equations.
\startplaceformula[reference=cv:dimen:relations]
\startformula
\startmathalignment
\NC \eta^{(l)}_x \NC = \eta^{(l-1)}_x - k^{(l)}_x + 1 \NR
\NC \eta^{(l)}_y \NC = \eta^{(l-1)}_y - k^{(l)}_y + 1 \NR
\stopmathalignment
\stopformula
\stopplaceformula
The equation governing the forward operation of these kinds of layers is:
\startplaceformula[reference=cv:forward:neuron]
\startformula
z^{(l)}_{(\color[red]{c})} (\color[red]{x}, \color[red]{y}) =
\sum_{\color[blue]{c'}}^{\eta^{(l-1)}_c}
\left(
    \sum_{\color[darkgreen]{x'}}^{k^{(l)}_x}
    \sum_{\color[darkgreen]{y'}}^{k^{(l)}_y}
    \Bigl(
        w^{(l)}_{(\color[red]{c}, \color[blue]{c'})}
            (\color[darkgreen]{x'}, \color[darkgreen]{y'})
        \, a^{(l-1)}_{(\color[blue]{c})}
            (\color[red]{x} + \color[darkgreen]{x'}, \color[red]{x} + \color[darkgreen]{y'})
    \Bigr)
\right)
+ b^{(l)}_{(\color[red]{c})}
\stopformula
\stopplaceformula

This equations tells us how to forward propagate through a convolutional layer but we can of course also use it to calculate how we are to backpropagate.
The backpropagation through this layer starts at its output with a gradient vector of matricies.
We are not going to try and write this vector of matricies down directly, it suffices to note that we have access to all partial derivatives of the form:
\startformula
\frac
    {
        \partial \ell
    }{
        \partial z^{(l)}_{(\color[red]{c})} (\color[red]{x}, \color[red]{y})
    }
\stopformula

The first partial derivative we need is with respect to the previous layers activation.
\startformula
\startmathalignment
\NC \frac
    {
        \partial z^{(l)}_{(\color[red]{c})} (\color[red]{x}, \color[red]{y})
    }{
        \partial a^{(l-1)}_{(\color[magenta]{c''})} (\color[magenta]{x''}, \color[magenta]{y''})
    }
\NC =
\frac
    {
        \partial 
    }{
        \partial a^{(l-1)}_{(\color[magenta]{c''})} (\color[magenta]{x''}, \color[magenta]{y''})
    }
\left(
    \sum_{\color[blue]{c'} = 0}^{\eta^{(l-1)}_c}
    \left(
        \sum_{\color[darkgreen]{x'} = 0}^{k^{(l)}_x}
        \sum_{\color[darkgreen]{y'} = 0}^{k^{(l)}_y}
        \Bigl(
            w^{(l)}_{(\color[red]{c}, \color[blue]{c'})}
                (\color[darkgreen]{x'}, \color[darkgreen]{y'})
            \, a^{(l-1)}_{(\color[blue]{c'})}
                (\color[red]{x} + \color[darkgreen]{x'}, \color[red]{x} + \color[darkgreen]{y'})
        \Bigr)
    \right)
    + b^{(l)}_{(\color[red]{c})}
\right)
\NR
\NC \NC =
\frac
    {
        \partial 
    }{
        \partial a^{(l-1)}_{(\color[magenta]{c''})} (\color[magenta]{x''}, \color[magenta]{y''})
    }
\left(
    \sum_{\color[darkgreen]{x'} = 0}^{k^{(l)}_x}
    \sum_{\color[darkgreen]{y'} = 0}^{k^{(l)}_y}
    \Bigl(
        w^{(l)}_{(\color[red]{c}, \color[magenta]{c''})}
            (\color[darkgreen]{x'}, \color[darkgreen]{y'})
        \, a^{(l-1)}_{(\color[magenta]{c''})}
            (\color[red]{x} + \color[darkgreen]{x'}, \color[red]{x} + \color[darkgreen]{y'})
    \Bigr)
\right)
\NR
\NC \NC =
w^{(l)}_{(\color[red]{c}, \color[magenta]{c''})} 
(\color[magenta]{x''} - \color[red]{x}, 
 \color[magenta]{y''} - \color[red]{y})
\NR
\stopmathalignment
\stopformula

With the chain rule we then obtain:
\startplaceformula[reference=fc:backward:neuron]
\startformula
\frac
    {
        \partial \ell
    }{
        \partial a^{(l-1)}_{(\color[magenta]{c''})} (\color[magenta]{x''}, \color[magenta]{y''})
    }
=
\sum_{\color[red]{c} = 0}^{\eta^{(l)}_c}
\sum_{\color[red]{x} = 0}^{\eta^{(l)}_x}
\sum_{\color[red]{y} = 0}^{\eta^{(l)}_y}
\frac
    {
        \partial \ell
    }{
        \partial z^{(l)}_{(\color[red]{c})} (\color[red]{x}, \color[red]{y})
    }
\frac
    {
        \partial z^{(l)}_{(\color[red]{c})} (\color[red]{x}, \color[red]{y})
    }{
        \partial a^{(l-1)}_{(\color[magenta]{c''})} (\color[magenta]{x''}, \color[magenta]{y''})
    }
\stopformula
\stopplaceformula

Next we need the partial derivative with respect to the weights in the layer.
\startformula
\startmathalignment
\NC \frac
    {
        \partial z^{(l)}_{(\color[red]{c})} (\color[red]{x}, \color[red]{y})
    }{
        \partial w^{(l)}_{(\color[red]{c}, \color[magenta]{c''})}
            (\color[darkyellow]{x''}, \color[darkyellow]{y''})
    }
\NC =
\frac
    {
        \partial
    }{
        \partial w^{(l)}_{(\color[red]{c}, \color[magenta]{c''})}
            (\color[darkyellow]{x''}, \color[darkyellow]{y''})
    }
\left(
    \sum_{\color[blue]{c'} = 0}^{\eta^{(l-1)}_c}
    \left(
        \sum_{\color[darkgreen]{x'} = 0}^{k^{(l)}_x}
        \sum_{\color[darkgreen]{y'} = 0}^{k^{(l)}_y}
        \Bigl(
            w^{(l)}_{(\color[red]{c}, \color[blue]{c'})}
                (\color[darkgreen]{x'}, \color[darkgreen]{y'})
            \, a^{(l-1)}_{(\color[blue]{c'})}
                (\color[red]{x} + \color[darkgreen]{x'}, \color[red]{x} + \color[darkgreen]{y'})
        \Bigr)
    \right)
    + b^{(l)}_{(\color[red]{c})}
\right)
\NR
\NC \NC =
\frac
    {
        \partial
    }{
        \partial w^{(l)}_{(\color[red]{c}, \color[magenta]{c''})}
            (\color[darkyellow]{x''}, \color[darkyellow]{y''})
    }
\left(
    \sum_{\color[darkgreen]{x'} = 0}^{k^{(l)}_x}
    \sum_{\color[darkgreen]{y'} = 0}^{k^{(l)}_y}
    \Bigl(
        w^{(l)}_{(\color[red]{c}, \color[magenta]{c''})}
            (\color[darkgreen]{x'}, \color[darkgreen]{y'})
        \, a^{(l-1)}_{(\color[magenta]{c''})}
            (\color[red]{x} + \color[darkgreen]{x'}, \color[red]{x} + \color[darkgreen]{y'})
    \Bigr)
\right)
\NR
\NC \NC =
a^{(l-1)}_{(\color[magenta]{c''})}
            (\color[red]{x} + \color[darkyellow]{x''}, \color[red]{x} + \color[darkyellow]{y''})
\stopmathalignment
\stopformula

With the chain rule we then obtain:
\startplaceformula
\startformula
\frac
    {
        \partial \ell
    }{
        \partial w^{(l)}_{(\color[red]{c}, \color[magenta]{c''})}
            (\color[darkyellow]{x''}, \color[darkyellow]{y''})
    }
=
\sum_{\color[red]{x} = 0}^{\eta^{(l)}_x}
\sum_{\color[red]{y} = 0}^{\eta^{(l)}_y}
\frac
    {
        \partial \ell
    }{
        \partial z^{(l)}_{(\color[red]{c})} (\color[red]{x}, \color[red]{y})
    }
\frac
    {
        \partial z^{(l)}_{(\color[red]{c})} (\color[red]{x}, \color[red]{y})
    }{
        \partial w^{(l)}_{(\color[red]{c}, \color[magenta]{c''})}
            (\color[darkyellow]{x''}, \color[darkyellow]{y''})
    }
\stopformula
\stopplaceformula

Repating the same calculation for the bias.
\startformula
\startmathalignment
\NC \frac
    {
        \partial z^{(l)}_{(\color[red]{c})} (\color[red]{x}, \color[red]{y})
    }{
        \partial b^{(l)}_{(\color[red]{c})}
    }
\NC =
\frac
    {
        \partial
    }{
        \partial b^{(l)}_{(\color[red]{c})}
    }
\left(
    \sum_{\color[blue]{c'} = 0}^{\eta^{(l-1)}_c}
    \left(
        \sum_{\color[darkgreen]{x'} = 0}^{k^{(l)}_x}
        \sum_{\color[darkgreen]{y'} = 0}^{k^{(l)}_y}
        \Bigl(
            w^{(l)}_{(\color[red]{c}, \color[blue]{c'})}
                (\color[darkgreen]{x'}, \color[darkgreen]{y'})
            \, a^{(l-1)}_{(\color[blue]{c'})}
                (\color[red]{x} + \color[darkgreen]{x'}, \color[red]{x} + \color[darkgreen]{y'})
        \Bigr)
    \right)
    + b^{(l)}_{(\color[red]{c})}
\right)
\NR
\NC \NC = 1 \NR
\stopmathalignment
\stopformula

With the chain rule we then obtain:
\startplaceformula
\startformula
\frac
    {
        \partial \ell
    }{
        b^{(l)}_{(\color[red]{c})}
    }
=
\frac
    {
        \partial \ell
    }{
        \partial z^{(l)}_{(\color[red]{c})} (\color[red]{x}, \color[red]{y})
    }
\stopformula
\stopplaceformula




It is possible to \quote{scale up} equation~\in[cv:forward:neuron] to the \quote{layer level} like we did for the fully-connected layer.
We do however, need a rather complicated scheme in order to this.
Here we make use of various $h$ variables which represent the Hadamard product.
\startformula
\startmathalignment
\NC h^{(l)}_{(\color[red]{c}, \color[blue]{c'})} (\color[red]{x}, \color[red]{y}) \NC =
\sum_{\color[darkgreen]{x'}}^{k^{(l)}_x}
\sum_{\color[darkgreen]{y'}}^{k^{(l)}_y}
\Bigl(
    w^{(l)}_{(\color[red]{c}, \color[blue]{c'})}
        (\color[darkgreen]{x'}, \color[darkgreen]{y'})
    \, a^{(l-1)}_{(\color[blue]{c'})}
        (\color[red]{x} + \color[darkgreen]{x'}, \color[red]{x} + \color[darkgreen]{y'})
\Bigr)
\NR
\NC h^{(l)}_{(\color[red]{c})} (\color[red]{x}, \color[red]{y}) \NC =
\sum_{\color[blue]{c'}}^{\eta^{(l-1)}_c}
h^{(l)}_{(\color[red]{c}, \color[blue]{c'})} (\color[red]{x}, \color[red]{y})
\NR
\NC H^{(l)}_{(\color[red]{c})} \NC =
\startmatrix[
    left={\left(},
    right={\right)},
]
    \NC h^{(l)}_{\color[red]{c}} (\color[red]{0}, \color[red]{0})
    \NC h^{(l)}_{\color[red]{c}} (\color[red]{0}, \color[red]{1})
    \NC \dots
    \NC h^{(l)}_{\color[red]{c}} (\color[red]{0}, \color[red]{\eta^{(l)}_y})
    \NR 
    \NC h^{(l)}_{\color[red]{c}} (\color[red]{1}, \color[red]{0})
    \NC h^{(l)}_{\color[red]{c}} (\color[red]{1}, \color[red]{1})
    \NC \dots
    \NC h^{(l)}_{\color[red]{c}} (\color[red]{1}, \color[red]{\eta^{(l)}_y})
    \NR 
    \NC \vdots
    \NC \vdots
    \NC \ddots
    \NC \vdots
    \NR 
    \NC h^{(l)}_{\color[red]{c}} (\color[red]{\eta^{(l)}_x}, \color[red]{0})
    \NC h^{(l)}_{\color[red]{c}} (\color[red]{\eta^{(l)}_x}, \color[red]{1})
    \NC \dots
    \NC h^{(l)}_{\color[red]{c}} (\color[red]{\eta^{(l)}_x}, \color[red]{\eta^{(l)}_y})
    \NR 
\stopmatrix
\NR
\NC Z^{(l)}_{(\color[red]{c})} \NC =
\startmatrix[
    left={\left(},
    right={\right)},
]
    \NC z^{(l)}_{\color[red]{c}} (\color[red]{0}, \color[red]{0})
    \NC z^{(l)}_{\color[red]{c}} (\color[red]{0}, \color[red]{1})
    \NC \dots
    \NC z^{(l)}_{\color[red]{c}} (\color[red]{0}, \color[red]{\eta^{(l)}_y})
    \NR 
    \NC z^{(l)}_{\color[red]{c}} (\color[red]{1}, \color[red]{0})
    \NC z^{(l)}_{\color[red]{c}} (\color[red]{1}, \color[red]{1})
    \NC \dots
    \NC z^{(l)}_{\color[red]{c}} (\color[red]{1}, \color[red]{\eta^{(l)}_y})
    \NR 
    \NC \vdots
    \NC \vdots
    \NC \ddots
    \NC \vdots
    \NR 
    \NC z^{(l)}_{\color[red]{c}} (\color[red]{\eta^{(l)}_x}, \color[red]{0})
    \NC z^{(l)}_{\color[red]{c}} (\color[red]{\eta^{(l)}_x}, \color[red]{1})
    \NC \dots
    \NC z^{(l)}_{\color[red]{c}} (\color[red]{\eta^{(l)}_x}, \color[red]{\eta^{(l)}_y})
    \NR 
\stopmatrix
\NR 
\NC B^{(l)}_{(\color[red]{c})} \NC =
\startmatrix[
    left={\left(},
    right={\right)},
]
    \NC b^{(l)}_{\color[red]{c}}
    \NC b^{(l)}_{\color[red]{c}}
    \NC \dots
    \NC b^{(l)}_{\color[red]{c}}
    \NR 
    \NC b^{(l)}_{\color[red]{c}}
    \NC b^{(l)}_{\color[red]{c}}
    \NC \dots
    \NC b^{(l)}_{\color[red]{c}}
    \NR 
    \NC \vdots
    \NC \vdots
    \NC \ddots
    \NC \vdots
    \NR 
    \NC b^{(l)}_{\color[red]{c}}
    \NC b^{(l)}_{\color[red]{c}}
    \NC \dots
    \NC b^{(l)}_{\color[red]{c}}
    \NR 
\stopmatrix
\NR
\stopmathalignment
\stopformula

As a last step we remove the channel indices.
\startformula
\startmathalignment
\NC {\bi H}^{(l)} \NC =
\startmatrix[
    left={\left(},
    right={\right)},
]
    \NC H^{(l)}_{(\color[red]{0})} 
    \NR
    \NC H^{(l)}_{(\color[red]{1})}
    \NR
    \NC \dots
    \NR
    \NC H^{(l)}_{(\color[red]{\eta^{(l)}_c})}
    \NR
\stopmatrix
\NR 
\NC {\bi Z}^{(l)} \NC =
\startmatrix[
    left={\left(},
    right={\right)},
]
    \NC Z^{(l)}_{(\color[red]{0})}
    \NR
    \NC Z^{(l)}_{(\color[red]{1})}
    \NR
    \NC \dots
    \NR
    \NC Z^{(l)}_{(\color[red]{\eta^{(l)}_c})}
    \NR
\stopmatrix
\NR 
\NC {\bi B}^{(l)} \NC =
\startmatrix[
    left={\left(},
    right={\right)},
]
    \NC B^{(l)}_{(\color[red]{0})}
    \NR
    \NC B^{(l)}_{(\color[red]{1})}
    \NR
    \NC \dots
    \NR
    \NC B^{(l)}_{(\color[red]{\eta^{(l)}_c})}
    \NR
\stopmatrix
\NR
\stopmathalignment
\stopformula

With this scheme we can write equation~\in[cv:forward:neuron] as:
\startplaceformula[reference=cv:forward:layer]
\startformula
{\bi Z}^{(l)} = {\bi H}^{(l)} + {\bi B}^{(l)}
\stopformula
\stopplaceformula
\stopsubsubject