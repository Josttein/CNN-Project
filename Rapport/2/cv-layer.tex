The convolutional layer is in many ways an extension of the regular fully-connected layer.
One of the issues with regular fully-connected layers is that these kinds of layers only accept input that is in the form of a vector.
This means that for applications where it is not natural for the input to be in a vector format, say image recognition, the input first has to be translated to a vector format.
Usually this results in a loss of information contained in the input.
In the typical case of image recongition, the input is in the form of one or more arrays of two dimensions.
For a multilayer percepetron to treat this input, the images has to be \quote{flattend} into a vector of one dimention before it can be passed on to the network.
This procedure eliminates some of the pixel relations in the image.
To deduce this, consider the process of reconstructing a flattend image.
If the image's dimensions prior to being flattend is not known, it is impossible, without the aid of pattern recognition, to reconstruct the image and be sure the reconstruction is equal to the original image.

To fix this problem, we can use a \quote{simple} solution.
Instead of having the neuron contain a vector of weights, let it have an array of weights.
These weight arrays are often called the neurons {\em kernel} or {\em filter}.
If we change the neuron's vector of weights into an array, we also need to change the operation that is used to combine the weights with the input (which in a neuron of a multilayer percepetron is the dot product).
There are two things to consider here.
The purpose of the weights is to look for features or {\em patterns} in the input---by emphasizing or deemphasizing certain aspects---and the operation must reflect this purpose of the weights.
Furthermore, the result of the operation should be a single number which, in a sense, representes the neurons \quote{initial} confidence that the feature it is looking for is present in the input.
The operation which does both of these things is the {\em Hadamard product}.
The Hadamard product can be seen as an extension of the dot product for two dimentional arrays.
It combines two arrays---of the same dimensions---by multiplying corresponding entries together and summing the results.
Which is precisly what the dot product does with two vectors.

Another thing we need to take into account is the dimentions of our new weight array.
If we were to proceed analogically to how a multilayer percepetron works, the array should have the same size as the input to the neuron.
\quote{Connecting} each value in the input to an individual weight in the neuron.
But this approach means that each neuron, in principle, looks for a single feature in the entire input at once.
A more refined approach, is to let the neuron's weight array be smaller in size than its input.
Instead of applying the weight array (using the Hadamard product) on the entire input at once, we apply it (still using the Hadamard product) to portions of the input seperately.
Intuitivly, this means that the weight arrays are \quote{scanned} across the entire input image.
Figure~\in[conv-operation] illustrates this idea of the weight array scanning the input matrix to produce the output matrix.

A weight array that is smaller than the input matrix allows the neurons to be trained to look for a single small feature, such as a \quote{sharp edge} or a \quote{round corner}.
Which in can look for in multiple areas of the input.
As figure~\in[conv-operation] clearly shows, the output of the neurons is no longer be a single number representing how \quote{initialy} confident the neuron is that a particular feature is present in the input {\em\bf as a whole}.
Rather, the result becomes a {\em feature map}.
Another two dimentional array which representes how \quote{initialy} confident the neuron is that a particular feature is present in {\em\bf specific locations} of the input.

\startplacefigure[
    reference=conv-operation,
    title={The basic forward operation of a convolutional layer~\cite[escontrela_2018].},
    location=bottom,
]
\startcombination[3*3]
{\externalfigure[./Images/conv-0.jpg][width=.3\textwidth]}{}
{\externalfigure[./Images/conv-1.jpg][width=.3\textwidth]}{}
{\externalfigure[./Images/conv-2.jpg][width=.3\textwidth]}{}
{\externalfigure[./Images/conv-3.jpg][width=.3\textwidth]}{}
{\externalfigure[./Images/conv-4.jpg][width=.3\textwidth]}{}
{\externalfigure[./Images/conv-5.jpg][width=.3\textwidth]}{}
{\externalfigure[./Images/conv-6.jpg][width=.3\textwidth]}{}
{\externalfigure[./Images/conv-7.jpg][width=.3\textwidth]}{}
{\externalfigure[./Images/conv-8.jpg][width=.3\textwidth]}{}
\stopcombination
\stopplacefigure

Of course each neuron must be able to handle more than one input data stream each with its own input data matrix.
A typical example of a case where multiple streams are needed is the treatment of RGB images.
An RGB image consists of three arrays of pixel values (numbers) that desribe how red, green and blue an image is in each pixel.
Such an input would be split into three channels before being passed to a convolutional layer.
The neurons handle multiple data streams by simple adding together the feature maps produces by each data stream.
Figure~\in[cv-network] gives an overview of a convolutional layer.
The channels are drawn as pipes to emhpasize that they contain matrix data rather than scalar values.

\startplacefigure[
    reference=cv-network,
    location=bottom,
    title={A diagram of a convolutional layer},
]
\externalfigure[Images/cv-network.pdf][width=.7\textwidth]
\stopplacefigure

\startsubsubsection[title=Strides and zero padding]
There are two other aspects of convolutional layers that we have yet to discuss: strides and zero padding.
The stride is the number of rows/columns that the kernel moves over the input matrix between each scanning.
Figure~\in[conv-operation] shows a scenario where the stride is equal to one.

What purpose does altering the stride serve?
Increasing the stride reduces the size of the resulting feature map since it is computed at fewer areas.
Which has the net effect that the {\em computational complexity} of the neuron is reduced.
However, it also means that the neuron studies its input less which can result in vital information in the input being overlooked.
This is not a big problem as long as the features of the input is judged to be larger than the kernel that the neuron uses.
So long as this is the case, increasing the stride of a neuron can significantly reduce its computational complexity without resulting in it no longer surving its purpose.

Let us now further develop Eq.~(\in[devel-feature-4]) to take into account variable strides.
It is as simple as multiplying the red indices with the strides.

\startplaceformula[reference=devel-feature-5]
\startformula
F(\color[red]{x},\color[red]{y})
=
{\rm activation}
\left( 
\sum_{\color[blue]{y}}^{}
\sum_{\color[blue]{x}}^{}
\left(
\sum_{c}^{}
\Bigl(
K(\color[blue]{x},\color[blue]{y},c)
\cdot
M(\color[red]{x}s_x + \color[blue]{x}, \color[red]{y}s_y + \color[blue]{y},c)
\right)
+
b
\right)
\stopformula
\stopplaceformula
\stopsubsubsection

\startsubsubsection[title=Zero padding]
If we increase a neurons stride we can shrink the size of the resulting feature map.
But it does not allow us to increase the size of the feature map!
To do this, a techinque known as {\em zero padding} is used.
Instead of working with the input {\em as is}, one or more extra outer rows and columns of zeros are added onto it before the kernel is scanned across it.
This allows us to increase the size of the resulting feature map.

This techinque is predominantly used to make the feature map have the same dimentions as the input image in applicatons where this is desirable.
\stopsubsubsection


For the remainder of this section $l$ is the index of a convolutional layer.

The dimentions of the input matricies and the kernel directly determine the dimentions of the output matricies.
\startplaceformula[reference=cv:dimen:relations]
\startformula
\startmathalignment
\NC \eta^{(l)}_x \NC = \eta^{(l-1)}_x - k^{(l)}_x + 1 \NR
\NC \eta^{(l)}_y \NC = \eta^{(l-1)}_y - k^{(l)}_y + 1 \NR
\stopmathalignment
\stopformula
\stopplaceformula
The equation governing the forward operation for this layer at the level of the individual neuron is:
\startplaceformula[reference=cv:forward:neuron]
\startformula
z^{(l)}_{(\color[red]{c})} (\color[red]{x}, \color[red]{y}) =
\sum_{\color[blue]{c'}}^{\eta^{(l-1)}_c}
\left(
    \sum_{\color[darkgreen]{x'}}^{k^{(l)}_x}
    \sum_{\color[darkgreen]{y'}}^{k^{(l)}_y}
    \Bigl(
        w^{(l)}_{(\color[red]{c}, \color[blue]{c'})}
            (\color[darkgreen]{x'}, \color[darkgreen]{y'})
        \, a^{(l-1)}_{(\color[blue]{c})}
            (\color[red]{x} + \color[darkgreen]{x'}, \color[red]{x} + \color[darkgreen]{y'})
    \Bigr)
\right)
+ b^{(l)}_{(\color[red]{c})}
\stopformula
\stopplaceformula
This equation has so much detail in it that scaling it up to the layer level necessarly removes most of the information in it.
But with it is possible with the following scheme which makes use of the intermediary $h$ variables.
\startformula
\startmathalignment
\NC h^{(l)}_{(\color[red]{c}, \color[blue]{c'})} (\color[red]{x}, \color[red]{y}) \NC =
\sum_{\color[darkgreen]{x'}}^{k^{(l)}_x}
\sum_{\color[darkgreen]{y'}}^{k^{(l)}_y}
\Bigl(
    w^{(l)}_{(\color[red]{c}, \color[blue]{c'})}
        (\color[darkgreen]{x'}, \color[darkgreen]{y'})
    \, a^{(l-1)}_{(\color[blue]{c'})}
        (\color[red]{x} + \color[darkgreen]{x'}, \color[red]{x} + \color[darkgreen]{y'})
\Bigr)
\NR
\NC h^{(l)}_{(\color[red]{c})} (\color[red]{x}, \color[red]{y}) \NC =
\sum_{\color[blue]{c'}}^{\eta^{(l-1)}_c}
h^{(l)}_{(\color[red]{c}, \color[blue]{c'})} (\color[red]{x}, \color[red]{y})
\NR
\NC H^{(l)}_{(\color[red]{c})} \NC =
\startmatrix[
    left={\left(},
    right={\right)},
]
    \NC h^{(l)}_{\color[red]{c}} (\color[red]{0}, \color[red]{0})
    \NC h^{(l)}_{\color[red]{c}} (\color[red]{0}, \color[red]{1})
    \NC \dots
    \NC h^{(l)}_{\color[red]{c}} (\color[red]{0}, \color[red]{\eta^{(l)}_y})
    \NR 
    \NC h^{(l)}_{\color[red]{c}} (\color[red]{1}, \color[red]{0})
    \NC h^{(l)}_{\color[red]{c}} (\color[red]{1}, \color[red]{1})
    \NC \dots
    \NC h^{(l)}_{\color[red]{c}} (\color[red]{1}, \color[red]{\eta^{(l)}_y})
    \NR 
    \NC \vdots
    \NC \vdots
    \NC \ddots
    \NC \vdots
    \NR 
    \NC h^{(l)}_{\color[red]{c}} (\color[red]{\eta^{(l)}_x}, \color[red]{0})
    \NC h^{(l)}_{\color[red]{c}} (\color[red]{\eta^{(l)}_x}, \color[red]{1})
    \NC \dots
    \NC h^{(l)}_{\color[red]{c}} (\color[red]{\eta^{(l)}_x}, \color[red]{\eta^{(l)}_y})
    \NR 
\stopmatrix
\NR
\NC Z^{(l)}_{(\color[red]{c})} \NC =
\startmatrix[
    left={\left(},
    right={\right)},
]
    \NC z^{(l)}_{\color[red]{c}} (\color[red]{0}, \color[red]{0})
    \NC z^{(l)}_{\color[red]{c}} (\color[red]{0}, \color[red]{1})
    \NC \dots
    \NC z^{(l)}_{\color[red]{c}} (\color[red]{0}, \color[red]{\eta^{(l)}_y})
    \NR 
    \NC z^{(l)}_{\color[red]{c}} (\color[red]{1}, \color[red]{0})
    \NC z^{(l)}_{\color[red]{c}} (\color[red]{1}, \color[red]{1})
    \NC \dots
    \NC z^{(l)}_{\color[red]{c}} (\color[red]{1}, \color[red]{\eta^{(l)}_y})
    \NR 
    \NC \vdots
    \NC \vdots
    \NC \ddots
    \NC \vdots
    \NR 
    \NC z^{(l)}_{\color[red]{c}} (\color[red]{\eta^{(l)}_x}, \color[red]{0})
    \NC z^{(l)}_{\color[red]{c}} (\color[red]{\eta^{(l)}_x}, \color[red]{1})
    \NC \dots
    \NC z^{(l)}_{\color[red]{c}} (\color[red]{\eta^{(l)}_x}, \color[red]{\eta^{(l)}_y})
    \NR 
\stopmatrix
\NR 
\NC B^{(l)}_{(\color[red]{c})} \NC =
\startmatrix[
    left={\left(},
    right={\right)},
]
    \NC b^{(l)}_{\color[red]{c}}
    \NC b^{(l)}_{\color[red]{c}}
    \NC \dots
    \NC b^{(l)}_{\color[red]{c}}
    \NR 
    \NC b^{(l)}_{\color[red]{c}}
    \NC b^{(l)}_{\color[red]{c}}
    \NC \dots
    \NC b^{(l)}_{\color[red]{c}}
    \NR 
    \NC \vdots
    \NC \vdots
    \NC \ddots
    \NC \vdots
    \NR 
    \NC b^{(l)}_{\color[red]{c}}
    \NC b^{(l)}_{\color[red]{c}}
    \NC \dots
    \NC b^{(l)}_{\color[red]{c}}
    \NR 
\stopmatrix
\NR
\NC {\bi H}^{(l)} \NC =
\startmatrix[
    left={\left(},
    right={\right)},
]
    \NC H^{(l)}_{(\color[red]{0})} 
    \NR
    \NC H^{(l)}_{(\color[red]{1})}
    \NR
    \NC \dots
    \NR
    \NC H^{(l)}_{(\color[red]{\eta^{(l)}_c})}
    \NR
\stopmatrix
\NR 
\NC {\bi Z}^{(l)} \NC =
\startmatrix[
    left={\left(},
    right={\right)},
]
    \NC Z^{(l)}_{(\color[red]{0})}
    \NR
    \NC Z^{(l)}_{(\color[red]{1})}
    \NR
    \NC \dots
    \NR
    \NC Z^{(l)}_{(\color[red]{\eta^{(l)}_c})}
    \NR
\stopmatrix
\NR 
\NC {\bi B}^{(l)} \NC =
\startmatrix[
    left={\left(},
    right={\right)},
]
    \NC B^{(l)}_{(\color[red]{0})}
    \NR
    \NC B^{(l)}_{(\color[red]{1})}
    \NR
    \NC \dots
    \NR
    \NC B^{(l)}_{(\color[red]{\eta^{(l)}_c})}
    \NR
\stopmatrix
\NR
\stopmathalignment
\stopformula

With this scheme we can write equation~\in[cv:forward:neuron] as:
\startplaceformula[reference=cv:forward:layer]
\startformula
{\bi Z}^{(l)} = {\bi H}^{(l)} + {\bi B}^{(l)}
\stopformula
\stopplaceformula

Now that we know how to move forwards through a convolutional layer we need to be able to propagate backwars through it as well.
The backpropagation through this layer starts at its output with a gradient vector of matricies.
We are not going to try and write this vector of matricies down directly, it suffices to note that we have access to all partial derivatives of the form:
\startformula
\frac
    {
        \partial \ell
    }{
        \partial z^{(l)}_{(\color[red]{c})} (\color[red]{x}, \color[red]{y})
    }
\stopformula

Using equation~\in[cv:forward:neuron] we can calculate the equations governing backpropagation.
\startformula
\startmathalignment
\NC \frac
    {
        \partial z^{(l)}_{(\color[red]{c})} (\color[red]{x}, \color[red]{y})
    }{
        \partial a^{(l-1)}_{(\color[magenta]{c''})} (\color[magenta]{x''}, \color[magenta]{y''})
    }
\NC =
\frac
    {
        \partial 
    }{
        \partial a^{(l-1)}_{(\color[magenta]{c''})} (\color[magenta]{x''}, \color[magenta]{y''})
    }
\left(
    \sum_{\color[blue]{c'} = 0}^{\eta^{(l-1)}_c}
    \left(
        \sum_{\color[darkgreen]{x'} = 0}^{k^{(l)}_x}
        \sum_{\color[darkgreen]{y'} = 0}^{k^{(l)}_y}
        \Bigl(
            w^{(l)}_{(\color[red]{c}, \color[blue]{c'})}
                (\color[darkgreen]{x'}, \color[darkgreen]{y'})
            \, a^{(l-1)}_{(\color[blue]{c'})}
                (\color[red]{x} + \color[darkgreen]{x'}, \color[red]{x} + \color[darkgreen]{y'})
        \Bigr)
    \right)
    + b^{(l)}_{(\color[red]{c})}
\right)
\NR
\NC \NC =
\frac
    {
        \partial 
    }{
        \partial a^{(l-1)}_{(\color[magenta]{c''})} (\color[magenta]{x''}, \color[magenta]{y''})
    }
\left(
    \sum_{\color[darkgreen]{x'} = 0}^{k^{(l)}_x}
    \sum_{\color[darkgreen]{y'} = 0}^{k^{(l)}_y}
    \Bigl(
        w^{(l)}_{(\color[red]{c}, \color[magenta]{c''})}
            (\color[darkgreen]{x'}, \color[darkgreen]{y'})
        \, a^{(l-1)}_{(\color[magenta]{c''})}
            (\color[red]{x} + \color[darkgreen]{x'}, \color[red]{x} + \color[darkgreen]{y'})
    \Bigr)
\right)
\NR
\NC \NC =
w^{(l)}_{(\color[red]{c}, \color[magenta]{c''})} 
(\color[magenta]{x''} - \color[red]{x}, 
 \color[magenta]{y''} - \color[red]{y})
\NR
\stopmathalignment
\stopformula

With the chain rule we then obtain:
\startplaceformula[reference=fc:backward:neuron]
\startformula
\frac
    {
        \partial \ell
    }{
        \partial a^{(l-1)}_{(\color[magenta]{c''})} (\color[magenta]{x''}, \color[magenta]{y''})
    }
=
\sum_{\color[red]{c} = 0}^{\eta^{(l)}_c}
\sum_{\color[red]{x} = 0}^{\eta^{(l)}_x}
\sum_{\color[red]{y} = 0}^{\eta^{(l)}_y}
\frac
    {
        \partial \ell
    }{
        \partial z^{(l)}_{(\color[red]{c})} (\color[red]{x}, \color[red]{y})
    }
\frac
    {
        \partial z^{(l)}_{(\color[red]{c})} (\color[red]{x}, \color[red]{y})
    }{
        \partial a^{(l-1)}_{(\color[magenta]{c''})} (\color[magenta]{x''}, \color[magenta]{y''})
    }
\stopformula
\stopplaceformula

\indentation
Equation~\in[cv:forward:neuron] also lets us calculate the partial derivaties with respect to all the weights and the bias in the layer.
\startformula
\startmathalignment
\NC \frac
    {
        \partial z^{(l)}_{(\color[red]{c})} (\color[red]{x}, \color[red]{y})
    }{
        \partial w^{(l)}_{(\color[red]{c}, \color[magenta]{c''})}
            (\color[darkyellow]{x''}, \color[darkyellow]{y''})
    }
\NC =
\frac
    {
        \partial
    }{
        \partial w^{(l)}_{(\color[red]{c}, \color[magenta]{c''})}
            (\color[darkyellow]{x''}, \color[darkyellow]{y''})
    }
\left(
    \sum_{\color[blue]{c'} = 0}^{\eta^{(l-1)}_c}
    \left(
        \sum_{\color[darkgreen]{x'} = 0}^{k^{(l)}_x}
        \sum_{\color[darkgreen]{y'} = 0}^{k^{(l)}_y}
        \Bigl(
            w^{(l)}_{(\color[red]{c}, \color[blue]{c'})}
                (\color[darkgreen]{x'}, \color[darkgreen]{y'})
            \, a^{(l-1)}_{(\color[blue]{c'})}
                (\color[red]{x} + \color[darkgreen]{x'}, \color[red]{x} + \color[darkgreen]{y'})
        \Bigr)
    \right)
    + b^{(l)}_{(\color[red]{c})}
\right)
\NR
\NC \NC =
\frac
    {
        \partial
    }{
        \partial w^{(l)}_{(\color[red]{c}, \color[magenta]{c''})}
            (\color[darkyellow]{x''}, \color[darkyellow]{y''})
    }
\left(
    \sum_{\color[darkgreen]{x'} = 0}^{k^{(l)}_x}
    \sum_{\color[darkgreen]{y'} = 0}^{k^{(l)}_y}
    \Bigl(
        w^{(l)}_{(\color[red]{c}, \color[magenta]{c''})}
            (\color[darkgreen]{x'}, \color[darkgreen]{y'})
        \, a^{(l-1)}_{(\color[magenta]{c''})}
            (\color[red]{x} + \color[darkgreen]{x'}, \color[red]{x} + \color[darkgreen]{y'})
    \Bigr)
\right)
\NR
\NC \NC =
a^{(l-1)}_{(\color[magenta]{c''})}
            (\color[red]{x} + \color[darkyellow]{x''}, \color[red]{x} + \color[darkyellow]{y''})
\stopmathalignment
\stopformula

With the chain rule we then obtain:
\startplaceformula
\startformula
\frac
    {
        \partial \ell
    }{
        \partial w^{(l)}_{(\color[red]{c}, \color[magenta]{c''})}
            (\color[darkyellow]{x''}, \color[darkyellow]{y''})
    }
=
\sum_{\color[red]{x} = 0}^{\eta^{(l)}_x}
\sum_{\color[red]{y} = 0}^{\eta^{(l)}_y}
\frac
    {
        \partial \ell
    }{
        \partial z^{(l)}_{(\color[red]{c})} (\color[red]{x}, \color[red]{y})
    }
\frac
    {
        \partial z^{(l)}_{(\color[red]{c})} (\color[red]{x}, \color[red]{y})
    }{
        \partial w^{(l)}_{(\color[red]{c}, \color[magenta]{c''})}
            (\color[darkyellow]{x''}, \color[darkyellow]{y''})
    }
\stopformula
\stopplaceformula

Repating the same calculation for the bias.
\startformula
\startmathalignment
\NC \frac
    {
        \partial z^{(l)}_{(\color[red]{c})} (\color[red]{x}, \color[red]{y})
    }{
        \partial b^{(l)}_{(\color[red]{c})}
    }
\NC =
\frac
    {
        \partial
    }{
        \partial b^{(l)}_{(\color[red]{c})}
    }
\left(
    \sum_{\color[blue]{c'} = 0}^{\eta^{(l-1)}_c}
    \left(
        \sum_{\color[darkgreen]{x'} = 0}^{k^{(l)}_x}
        \sum_{\color[darkgreen]{y'} = 0}^{k^{(l)}_y}
        \Bigl(
            w^{(l)}_{(\color[red]{c}, \color[blue]{c'})}
                (\color[darkgreen]{x'}, \color[darkgreen]{y'})
            \, a^{(l-1)}_{(\color[blue]{c'})}
                (\color[red]{x} + \color[darkgreen]{x'}, \color[red]{x} + \color[darkgreen]{y'})
        \Bigr)
    \right)
    + b^{(l)}_{(\color[red]{c})}
\right)
\NR
\NC \NC = 1 \NR
\stopmathalignment
\stopformula

With the chain rule we then obtain:
\startplaceformula
\startformula
\frac
    {
        \partial \ell
    }{
        b^{(l)}_{(\color[red]{c})}
    }
=
\frac
    {
        \partial \ell
    }{
        \partial z^{(l)}_{(\color[red]{c})} (\color[red]{x}, \color[red]{y})
    }
\stopformula
\stopplaceformula


A convolutional neural network is an evolution of a classical multilayer percepetron network.
Recall the basic principle underpinning how a normal neuron in a neural network is supposed to work.
The neuron is supposed to \quote{look} for features in its input data.
If the neuron \quote{thinks} that those features are present in the input data it \quote{fires}.
Otherwise the neuron does not fire.

In a classical multilayer percepetron network this is implemented in the following way.
Each neuron contains a vector of weights, a bias and an activation function.
The input to the neuron---which must be a vector of equal length to the neuron's own weight vector---is combined with the neuron's weight vector using the dot product.
The neuron's bias is added onto the result which in turn is passed to the activation function which finally determines if the neuron \quote{fires} or not.

Using several layers of neurons one can achive quite remarkable results using this implementation of a neural network.
However, a multilayer percepetron is inherently limited.
The major problem is that neurons in these kinds of networks only accept input that is in the form of a vector.
This means that for applications where it is not natural for the input to be in a vector format, say image recognition, the input first has to be translated to a vector format.
Usually this results in a loss of information contained in the input.
In the typical case of image recongition, the input is in the form of one or more arrays of two dimentions.
For a multilayer percepetron to treat this input, the images has to be \quote{flattend} into a vector of one dimention before it can be passed on to the network.
This procedure eliminates some of the pixel relations in the image.
To deduce this, consider the process of reconstructing a flattend image.
If the image's dimentions prior to being flattend is not known, it is impossible, without the aid of pattern recognition, to reconstruct the image and be sure the reconstruction is equal to the original image.

To fix this problem, we can use a \quote{simple} solution.
Instead of having the neuron contain a vector of weights, let it have an array of weights.
If we change the neuron's vector of weights into an array, we also need to change the operation that is used to combine the weights with the input (which in a neuron of a multilayer percepetron is the dot product).
There are two things to consider here.
The purpose of the weights is to look for features or {\em patterns} in the input---by emphasizing or deemphasizing certain aspects---and the operation must reflect this purpose of the weights.
Furthermore, the result of the operation should be a single number which, in a sense, representes the neurons \quote{initial} confidence that the feature it is looking for is present in the input.
The operation which does both of these things is the {\em Hadamard product}.
The Hadamard product can be viewed as an extension of the dot product to two dimentional arrays.
It combines two arrays---of the same dimentions---by multiplying corresponding entries together and summing the results.
Which is precisly what the dot product does with two vectors.

The Hadamard product (${\rm Hp}$) of two matricies $A,B$ with entries $a_{i,j},b_{i,j}$ of equal dimentions is
\startplaceformula[reference=hdprod]
\startformula
{\rm Hp}(A,B) = \sum_{j} \sum_{i} a_{i,j} \cdot b_{i,j}
\stopformula
\stopplaceformula

\indentation
Another thing we need to take into account is the dimentions of our new weight array.
If we were to proceed analogically to how a multilayer percepetron works, the array should have the same size as the input to the neuron.
\quote{Connecting} each value in the input to an individual weight in the neuron.
But this approach means that each neuron, in principle, looks for a single feature in the entire input at once.
A more refined approach, is to let the neuron's weight array be smaller in size than its input.
Instead of applying the weight array (using the Hadamard product) to the entire input at once, we apply it (still using the Hadamard product) to portions of the input seperately.
Intuitivly, this means that the weight arrays are \quote{scanned} across the entire input image.
This allows the neuron to be trained to look for a single feature, such as a \quote{sharp edge} or a \quote{round corner}, in multiple areas of the input.
The result of this method will no longer be a single number representing how \quote{initialy} confident the neuron is that a particular feature is present in the input {\em\bf as a whole}.
Rather, the result becomes a {\em feature map}.
Another two dimentional array which representes how \quote{initialy} confident the neuron is that a particular feature is present in {\em\bf specific locations} of the input.

Going one step further, we can allow the neuron to treat inputs of not only a single two dimentional array, but several two dimentional arrays.
A typical example where the input would consists of several interlinked two dimentional arrays is RGB images.
An RGB image consists of three arrays of pixel values (numbers) that desribe how red, green and blue an image is in each pixel.
The number of interlinked two dimentional arrays present in the input, is known as the number of {\em channels} that the input has.
In order for our neuron to treat inputs with more than one channel we let the neuron have as many channels as the input.
That is to say, we equip the neuron with a weight array for each channel in the input.
For each channel the weight arrays are applied to the input (using Hadamard) and the result in each channel is combined to form a final single feature map.

A neural network which makes use of layers of neurons of this kind, is a convolutional neural network.
The multi-channel two dimentional arrays of weights inside each such neuron is called the neuron's {\em kernel} or {\em filter}.
Why are these neural networks called convolutional neural networks?
That will be explained in the next section.

\startsubsection[title=The convolutional neuron]
Let us start this section with a simple convolutional neuron.
The neuron's kernel consists of a single weight matrix (one channel), some bias and some activation function.
Consequently, the input that this neuron accepts is any one channel two dimentional array of size greater than its kernel.

The process of calcuating the neurons \quote{initial} confidence that the feature it is looking for is present in the input, is illustrated by Figure~\in[conv-operation] on page~\at[conv-operation].
Figure~\in[conv-operation] shows how the kernel \quote{scans} the entire input array to compute the feature map.
Let us construct the general formula for this feature map.

We will denote the feature map, kernel and input as $F$, $K$ and $M$ respectivly.
They are two dimentional arrays and their individual entries will be denoted as $F(x,y)$ where $x$ is the column index and $y$ the row index.
The indexes will all start at zero (e.g.\ $M(1,2)$ is the entry in the second column, third row of $M$).
There are many lengths involved in working with three seperate arrays, various lengths will here be denoted by $n$.
A subscript, either $x$ or $y$, will indicate if it is a horisontal or vertical length and a subsequent argument will indicate which array the length belongs to.
So for example, $n_x(M)$ is the horisontal length of the input array---the number of columns or x's in $M$.

If the input represents a typical MNIST image with dimentions $28{\rm x}28$, then $n_x(M) = 28$ and $n_y(M) = 28$.
In this scenario, the indices of $M$ range in $[0 \, .. \, n_x(M) - 1] \times [0 \, .. \, n_y(M) - 1]$.

Let us say that the kernel scans its input one column and one row at a time.
In this scenario, the output feature map will have dimentions
\stopsubsection

\startsubsection[title=The convolutional layer]
Okay this is where I will start developing the mathematics.
First we need to define the various variables used and some terminology.
A network has several layers and each layer has its own specific attributes.
A supersricpt encolsed in parantheses will be used as the layer index.


\startplaceformula[reference=dimen-feature-1]
\startformula
\startmathalignment[n=1]
\NC \delta x_Z^{(l+1)} = \delta x_W^{(l)} - \delta x_Z^{(l)} + 1 \NR
\NC y_Z^{(l+1)} = \delta y_W^{(l)} - \delta y_Z^{(l)} + 1 \NR
\stopmathalignment
\stopformula
\stopplaceformula

\startplaceformula
\startformula
\startmathalignment[n=2]
\NC \color[red]{x} \in [0 \, .. \, n_x(F) - 1] \qquad \NC \color[red]{y} \in [0 \, .. \, n_y(F) - 1] \NR
\NC \color[blue]{x} \in [0 \, .. \, n_x(K) - 1] \qquad \NC \color[blue]{y} \in [0 \, .. \, n_y(K) - 1] \NR
\stopmathalignment
\stopformula
\stopplaceformula

\indentation
The product in layer $l$ before activation, here denoted $\color[blue]{Z}$ is given by the following formula


Regular formula
\startplaceformula[reference=devel-feature-1]
\startformula
\color[green]{Z}_{\color[red]{x},\color[red]{y},\color[red]{c}}^{(l+1)}
=
\sum_{\color[blue]{c'} = 0}^{\delta c_Z^{(l)}}
\sum_{\color[blue]{y'} = 0}^{\delta y_Z^{(l)}}
\sum_{\color[blue]{x'} = 0}^{\delta x_Z^{(l)}}
\Bigl(
W_{\color[blue]{x'}, \color[blue]{y'}, \color[blue]{c'}}^{(l, \color[red]{c})}
Z_{\color[red]{x} + \color[blue]{x'}, \color[red]{y} + \color[blue]{y'}, \color[blue]{c'}}^{(l)}
\Bigr)
+
b^{(l,\color[red]{c})}
\stopformula
\stopplaceformula

Derivative with respect to $Z$.
$\color[magenta]{c^*} \in [\color[red]{c} \, .. \, \color[red]{c} + \delta c_W^{(l)}]$.
$\color[magenta]{y^*} \in [\color[red]{y} \, .. \, \color[red]{y} + \delta y_W^{(l)}]$.
$\color[magenta]{x^*} \in [\color[red]{x} \, .. \, \color[red]{x} + \delta x_W^{(l)}]$.

\startplaceformula
\startformula
\startmathalignment
\NC \frac
   {\partial \color[green]{Z}_{\color[red]{x},\color[red]{y},\color[red]{c}}^{(l+1)}}
   {\partial Z_{\color[magenta]{x^*}, \color[magenta]{y^*}, \color[magenta]{c^*}}^{(l)}}
= \NC
\frac
   {\partial}
   {\partial Z_{\color[magenta]{x^*}, \color[magenta]{y^*}, \color[magenta]{c^*}}^{(l)}}
\left(
\sum_{\color[blue]{c'} = 0}^{\delta c_W^{(l)}}
\sum_{\color[blue]{y'} = 0}^{\delta y_W^{(l)}}
\sum_{\color[blue]{x'} = 0}^{\delta x_W^{(l)}}
\Bigl(
W_{\color[blue]{x'}, \color[blue]{y'}, \color[blue]{c'}}^{(l, \color[red]{c})}
Z_{\color[red]{x} + \color[blue]{x'}, \color[red]{y} + \color[blue]{y'}, \color[blue]{c'}}^{(l)}
\Bigr)
+
b^{(l,\color[red]{c})}
\right) \NR
\NC = \NC
\sum_{\color[blue]{c'} = 0}^{\delta c_W^{(l)}}
\sum_{\color[blue]{y'} = 0}^{\delta y_W^{(l)}}
\sum_{\color[blue]{x'} = 0}^{\delta x_W^{(l)}}
\frac
   {\partial}
   {\partial Z_{\color[magenta]{x^*}, \color[magenta]{y^*}, \color[magenta]{c^*}}^{(l)}}
\left(
\Bigl(
W_{\color[blue]{x'}, \color[blue]{y'}, \color[blue]{c'}}^{(l, \color[red]{c})}
Z_{\color[red]{x} + \color[blue]{x'}, \color[red]{y} + \color[blue]{y'}, \color[blue]{c'}}^{(l)}
\Bigr)
+
b^{(l,\color[red]{c})}
\right) \NR
\NC = \NC
W_{\color[blue]{x'}, \color[blue]{y'}, \color[blue]{c'}}^{(l, \color[red]{c})}
\stopmathalignment
\stopformula
\stopplaceformula

Derivative with respecto to $W$.
$\color[magenta]{c^*} \in [0 \, .. \, \delta c_W^{(l)}]$.
$\color[magenta]{y^*} \in [0 \, .. \, \delta y_W^{(l)}]$.
$\color[magenta]{x^*} \in [0 \, .. \, \delta x_W^{(l)}]$.

\startplaceformula
\startformula
\startmathalignment
\NC \frac
   {\partial \color[green]{Z}_{\color[red]{x},\color[red]{y},\color[red]{c}}^{(l+1)}}
   {\partial W_{\color[magenta]{x^*}, \color[magenta]{y^*}, \color[magenta]{c^*}}^{(l, \color[red]{c})}}
= \NC
\frac
   {\partial}
   {\partial W_{\color[magenta]{x^*}, \color[magenta]{y^*}, \color[magenta]{c^*}}^{(l, \color[red]{c})}}
\left(
\sum_{\color[blue]{c'} = 0}^{\delta c_W^{(l)}}
\sum_{\color[blue]{y'} = 0}^{\delta y_W^{(l)}}
\sum_{\color[blue]{x'} = 0}^{\delta x_W^{(l)}}
\Bigl(
W_{\color[blue]{x'}, \color[blue]{y'}, \color[blue]{c'}}^{(l, \color[red]{c})}
Z_{\color[red]{x} + \color[blue]{x'}, \color[red]{y} + \color[blue]{y'}, \color[blue]{c'}}^{(l)}
\Bigr)
+
b^{(l,\color[red]{c})}
\right) \NR
\NC = \NC
\sum_{\color[blue]{c'} = 0}^{\delta c_W^{(l)}}
\sum_{\color[blue]{y'} = 0}^{\delta y_W^{(l)}}
\sum_{\color[blue]{x'} = 0}^{\delta x_W^{(l)}}
\frac
   {\partial}
   {\partial W_{\color[magenta]{x^*}, \color[magenta]{y^*}, \color[magenta]{c^*}}^{(l, \color[red]{c})}}
\left(
\Bigl(
W_{\color[blue]{x'}, \color[blue]{y'}, \color[blue]{c'}}^{(l, \color[red]{c})}
Z_{\color[red]{x} + \color[blue]{x'}, \color[red]{y} + \color[blue]{y'}, \color[blue]{c'}}^{(l)}
\Bigr)
+
b^{(l,\color[red]{c})}
\right) \NR
\NC = \NC
Z_{\color[magenta]{x^*} + \color[red]{x'}, \color[magenta]{y^*} + \color[red]{y'}, \color[magenta]{c^*}}^{(l)} \NR
\stopmathalignment
\stopformula
\stopplaceformula

Derivative with respect to $b$
\startplaceformula
\startformula
\startmathalignment
\NC \frac
   {\partial \color[green]{Z}_{\color[red]{x},\color[red]{y},\color[red]{c}}^{(l+1)}}
   {\partial b^{(l,\color[red]{c})}}
= \NC
\frac
   {\partial}
   {\partial b^{(l,\color[red]{c})}}
\left(
\sum_{\color[blue]{c'} = 0}^{\delta c_W^{(l)}}
\sum_{\color[blue]{y'} = 0}^{\delta y_W^{(l)}}
\sum_{\color[blue]{x'} = 0}^{\delta x_W^{(l)}}
\Bigl(
W_{\color[blue]{x'}, \color[blue]{y'}, \color[blue]{c'}}^{(l, \color[red]{c})}
Z_{\color[red]{x} + \color[blue]{x'}, \color[red]{y} + \color[blue]{y'}, \color[blue]{c'}}^{(l)}
\Bigr)
+
b^{(l,\color[red]{c})}
\right) \NR
\NC = \NC 1 \NR
\stopmathalignment
\stopformula
\stopplaceformula

If we add a bias $b$ to this filter, the formula becomes

\startplaceformula[reference=devel-feature-2]
\startformula
F_*(\color[red]{x},\color[red]{y})
= 
\sum_{\color[blue]{y}}^{} 
\sum_{\color[blue]{x}}^{} 
\Bigl(
K(\color[blue]{x},\color[blue]{y})
\cdot
M(\color[red]{x} + \color[blue]{x}, \color[red]{y} + \color[blue]{y})
+
b
\Bigr)
\stopformula
\stopplaceformula

Sending this formula throug an activation function such as ReLU or the sigmoid function, we obtain the final output feature.

\startplaceformula[reference=devel-feature-3]
\startformula
F_*(\color[red]{x},\color[red]{y})
= 
{\rm activation}
\left(
\sum_{\color[blue]{y}}^{} 
\sum_{\color[blue]{x}}^{} 
\Bigl(
K(\color[blue]{x},\color[blue]{y})
\cdot
M(\color[red]{x} + \color[blue]{x}, \color[red]{y} + \color[blue]{y})
+
b
\Bigr)
\right)
\stopformula
\stopplaceformula

\startsubsubsection[title=Multiple channels]
Let us return to Eq.~(\in[devel-feature-3]) for a minute and ask: what happens if we add some channels to the input?
Let us denote the number of channels by $n_c$.
The input and kernel are now multi-channel arrays, their individual entries will be denoted as $K(i,j,c)$ with $c$ ranging in $[0 \, .. \, n_c - 1]$
To calculate the feature map when multiple channels are present, we have to take into account the contribution of each individual channel.
We do this by simply adding them together.
Since we only want to add the bias of the filter once per entry in the feature map, the resulting modified formula becomes

\startplaceformula[reference=devel-feature-4]
\startformula
F(\color[red]{x},\color[red]{y})
=
{\rm activation}
\left( 
\sum_{\color[blue]{y}}^{}
\sum_{\color[blue]{x}}^{}
\left(
\sum_{c}^{}
\Bigl(
K(\color[blue]{x},\color[blue]{y},c)
\cdot
M(\color[red]{x} + \color[blue]{x}, \color[red]{y} + \color[blue]{y},c)
\right)
+
b
\right)
\stopformula
\stopplaceformula
\stopsubsubsection