The fully-connected layer of a convolutional network is a classic neural network layer as seen in classical neural network.
Which are also known as multi-layer precepetrons.

Recall the concept of how neurons in a layer fire and such, here is how this concept is implemented in the fully-connected layer.
The layer takes as its input a multi-channel scalar stream of data.
Each such stream is connected to each neuron in the layer.
The neurons in the layer all have a vector of scalar weights equal in length to the number of input streams that the neurons have.
In addition, each neuron has its own bias.
The data in the input streams are multiplied with their corresponding weights in each neurons weight vector.
Subsequently, the bias is added onto this weighted sum.
This forms the preliminary output of the neurons ($z$) which is then sent through some activation function responsible for determining if the neuron fires or not.
Figure~\in[fc-handdraw] shows a diagram representing this kind of layer.

\startplacefigure[
    reference=fc-handdraw,
    location=bottom,
    title={A diagram of a fully-connected layer.},
]
\externalfigure[Images/fc-network.pdf][width=.7\textwidth]
\stopplacefigure

Since the data this layer is working with is scalar values there are no $\eta_x$ or $\eta_y$ to speak of.
Furthermore, the number of output streams is determined by the number of neurons in the layer.
Meaning that the number of channels in the previous layer is completely independent from the number of channels in the next layer.

Let $l$ be the index of a fully-connected layer.
The equation governing the forward operation for these kinds of layers is:
\startplaceformula[reference=fc:forward:neuron]
\startformula
z^{(l)}_{(\color[red]{c})} = 
\sum_{\color[blue]{c'} = 0}^{\eta^{(l-1)}_c}
\Bigl( 
        w^{(l)}_{(\color[red]{c}, \color[blue]{c'})}
        \, a^{(l-1)}_{(\color[blue]{c'})}
\Bigr) +
b^{(l)}_{(\color[red]{c})}
\stopformula
\stopplaceformula
The final output is obtained by sending $z$ through the chosen activation function.
Now that we know how to move forwards through a fully-connected layer we need to be able to propagate backwards through it as well.
We can use equation~\in[fc:forward:neuron] to calculate the equations governing the backward operation of these layers.
The backpropagation through this layer starts at its output with a gradient vector.
Recall that we denote the loss of the network by $\ell$ so we write this gradient vector as:
\startformula
\frac
    {
        \partial \ell
    }{
        \partial {\bi z}^{(l)}
    }
=
\startmatrix[
    left={\left(},
    right={\right)},
]
    \NC \frac
    {
        \partial \ell
    }{
        \partial z^{(l)}_{\color[red]{0}}
    }
    \NR
    \NC \frac
    {
        \partial \ell
    }{
        \partial z^{(l)}_{\color[red]{1}}
    }
    \NR
    \NC \vdots
    \NR
    \NC \frac
    {
        \partial \ell
    }{
        \partial z^{(l)}_{\color[red]{\eta^{(l)}_c}}
    }
    \NR 
\stopmatrix
\stopformula
The partial derivative which relates each $z$ to an $a$ in the previous layer is:
\startformula
\frac
    {
        \partial z^{(l)}_{(\color[red]{c})}
    }{
        \partial a^{(l-1)}_{(\color[magenta]{c''})}
    }
=
\frac
    {
        \partial
    }{
        \partial a^{(l-1)}_{(\color[magenta]{c''})}
    }
\left(
    \sum_{\color[blue]{c'} = 0}^{\eta^{(l-1)}_c}
    \Bigl( 
            w^{(l)}_{(\color[red]{c}, \color[blue]{c'})}
            \, a^{(l-1)}_{(\color[blue]{c'})}
    \Bigr) +
    b^{(l)}_{(\color[red]{c})}
\right)
=
w^{(l)}_{(\color[red]{c}, \color[magenta]{c''})}
\stopformula
Using the chain rule for partial derivatices we then obtain:
\startplaceformula[reference=fc:backward:layer]
\startformula
\frac
    {
        \partial \ell
    }{
        \partial a^{(l-1)}_{(\color[magenta]{c''})}
    }
=
\sum_{\color[red]{c} = 0}^{\eta^{(l)}_c}
\frac
    {
        \partial \ell
    }{
        \partial z^{(l)}_{(\color[red]{c})}
    }
\frac
    {
        \partial z^{(l)}_{\color[red]{c}}
    }{
        \partial a^{(l-1)}_{(\color[magenta]{c''})}
    }
=
\sum_{\color[red]{c} = 0}^{\eta^{(l)}_c}
\frac
    {
        \partial \ell
    }{
        \partial z^{(l)}_{(\color[red]{c})}
    }
w^{(l)}_{(\color[red]{c}, \color[magenta]{c''})}
\stopformula
\stopplaceformula

We also need to calculate the partial derivaties with respect to all the weights and the bias in the layer.
\startformula
\startmathalignment
\NC 
\frac
    {
        \partial z^{(l)}_{(\color[red]{c})}
    }{
        \partial w^{(l)}_{(\color[red]{c}, \color[magenta]{c''})}
    }
\NC =
\frac
    {
        \partial
    }{
        \partial w^{(l)}_{(\color[red]{c}, \color[magenta]{c''})}
    }
\left(
    \sum_{\color[blue]{c'} = 0}^{\eta^{(l-1)}_c}
    \Bigl( 
            w^{(l)}_{(\color[red]{c}, \color[blue]{c'})}
            \, a^{(l-1)}_{(\color[blue]{c'})}
    \Bigr) +
    b^{(l)}_{(\color[red]{c})}
\right)
=
a^{(l-1)}_{(\color[magenta]{c''})}
\NR 
\NC 
\frac
    {
        \partial z^{(l)}_{(\color[red]{c})}
    }{
        \partial b^{(l)}_{(\color[red]{c})}
    }
\NC =
\frac
    {
        \partial
    }{
        \partial b^{(l)}_{(\color[red]{c})}
    }
\left(
    \sum_{\color[blue]{c'} = 0}^{\eta^{(l-1)}_c}
    \Bigl( 
            w^{(l)}_{(\color[red]{c}, \color[blue]{c'})}
            \, a^{(l-1)}_{(\color[blue]{c'})}
    \Bigr) +
    b^{(l)}_{(\color[red]{c})}
\right)
=
1
\NR
\stopmathalignment
\stopformula
Applying the chain rule again we get:
\startplaceformula
\startformula
\startmathalignment
\NC 
\frac
    {
        \partial \ell
    }{
        \partial w^{(l)}_{(\color[red]{c}, \color[magenta]{c''})}
    }
\NC =
\frac
    {
        \partial \ell
    }{
        \partial z^{(l)}_{(\color[red]{c})}
    }
\frac
    {
        \partial z^{(l)}_{(\color[red]{c})}
    }{
        \partial w^{(l)}_{(\color[red]{c}, \color[magenta]{c''})}
    }
=
a^{(l-1)}_{(\color[magenta]{c''})}
\frac
    {
        \partial \ell
    }{
        \partial z^{(l)}_{(\color[red]{c})}
    }
\NR[reference=fc:backward:weight]
\NC 
\frac
    {
        \partial \ell
    }{
        \partial b^{(l)}_{(\color[red]{c})}
    }
\NC =
\frac
    {
        \partial \ell
    }{
        \partial z^{(l)}_{(\color[red]{c})}
    }
\frac
    {
        \partial z^{(l)}_{(\color[red]{c})}
    }{
        \partial b^{(l)}_{(\color[red]{c})}
    }
=
\frac
    {
        \partial \ell
    }{
        \partial z^{(l)}_{(\color[red]{c})}
    }
\NR[reference=fc:backward:bias]
\stopmathalignment
\stopformula
\stopplaceformula

We can with the following scheme \quote{scale up} equation~\in[fc:forward:neuron] to the \quote{layer level}.
\startformula
{\bi z}^{(l)} =
\startmatrix[
    left={\left(},
    right={\right)},
]
    \NC z^{(l)}_{(\color[red]{1})} \NR
    \NC z^{(l)}_{(\color[red]{2})} \NR
    \NC \vdots \NR
    \NC z^{(l)}_{(\color[red]{\eta^{(l)}_c})} \NR
\stopmatrix
\stopformula

\startformula
W^{(l)} = 
\startmatrix[
    left={\left(},
    right={\right)},
]
    \NC w^{(l)}_{(\color[red]{0}, \color[blue]{0})} 
    \NC w^{(l)}_{(\color[red]{0}, \color[blue]{1})}
    \NC \dots
    \NC w^{(l)}_{(\color[red]{0}, \color[blue]{\eta^{(l-1)}_c})}
    \NR 
    \NC w^{(l)}_{(\color[red]{1}, \color[blue]{0})} 
    \NC w^{(l)}_{(\color[red]{1}, \color[blue]{1})}
    \NC \dots
    \NC w^{(l)}_{(\color[red]{1}, \color[blue]{\eta^{(l-1)}_c})}
    \NR 
    \NC \vdots
    \NC \vdots
    \NC \ddots
    \NC \vdots
    \NR 
    \NC w^{(l)}_{(\color[red]{\eta^{(l)}_c}, \color[blue]{0})} 
    \NC w^{(l)}_{(\color[red]{\eta^{(l)}_c}, \color[blue]{1})}
    \NC \dots
    \NC w^{(l)}_{(\color[red]{\eta^{(l)}_c}, \color[blue]{\eta^{(l-1)}_c})}
    \NR 
\stopmatrix
\stopformula

\startformula
{\bi a}^{(l-1)} =
\startmatrix[
    left={\left(},
    right={\right)}]
    \NC a^{(l-1)}_{(\color[blue]{1})} \NR
    \NC a^{(l-1)}_{(\color[blue]{2})} \NR
    \NC \vdots \NR
    \NC a^{(l-1)}_{(\color[blue]{\eta^{(l-1)}_c})} \NR
\stopmatrix
\stopformula

\startformula
{\bi b}^{(l)} =
\startmatrix[
    left={\left(},
    right={\right)}]
    \NC b^{(l)}_{(\color[red]{0})} \NR
    \NC b^{(l)}_{(\color[red]{1})} \NR
    \NC \vdots \NR
    \NC b^{(l)}_{(\color[red]{\eta^{(l)}_c})} \NR
\stopmatrix
\stopformula
Now we can write equation~\in[fc:forward:neuron] as:
\startplaceformula[reference=fc:forward:layer]
\startformula
{\bi z}^{(l)} = 
W^{(l)}
{\bi a}^{(l-1)}
 +
{\bi b}^{(l)}
\stopformula
\stopplaceformula