\startsection[title={\em Convolutional} Neural Networks]
A convolutional neural network is an evolution of a classical multilayer percepetron network.
Recall the basic principle underpinning how a normal neuron in a neural network is supposed to work.
The neuron is supposed to \quote{look} for features in its input data.
If the neuron \quote{thinks} that those features are present in the input data it \quote{fires}.
Otherwise the neuron does not fire.

In a classical multilayer percepetron network this is implemented in the following way.
Each neuron contains a vector of weights, a bias and an activation function.
The input to the neuron---which must be a vector of equal length to the neuron's own weight vector---is combined with the neuron's weight vector using the dot product.
The neuron's bias is added onto the result which in turn is passed to the activation function which finally determines if the neuron \quote{fires} or not.

Using several layers of neurons one can achive quite remarkable results using this implementation of a neural network.
However, a multilayer percepetron is inherently limited.
The major problem is that neurons in these kinds of networks only accept input that is in the form of a vector.
This means that for applications where it is not natural for the input to be in a vector format, say image recognition, the input first has to be translated to a vector format.
Usually this results in a loss of information contained in the input.
In the typical case of image recongition, the input is in the form of one or more arrays of two dimentions.
For a multilayer percepetron to treat this input, the images has to be \quote{flattend} into a vector of one dimention before it can be passed on to the network.
This procedure eliminates some of the pixel relations in the image.
To deduce this, consider the process of reconstructing a flattend image.
If the image's dimentions prior to being flattend is not known, it is impossible, without the aid of pattern recognition, to reconstruct the image and be sure the reconstruction is equal to the original image.

To fix this problem, we can use a \quote{simple} solution.
Instead of having the neuron contain a vector of weights, let it have an array of weights.
If we change the neuron's vector of weights into an array, we also need to change the operation that is used to combine the weights with the input (which in a neuron of a multilayer percepetron is the dot product).
There are two things to consider here.
The purpose of the weights is to look for features or {\em patterns} in the input---by emphasizing or deemphasizing certain aspects---and the operation must reflect this purpose of the weights.
Furthermore, the result of the operation should be a single number which, in a sense, representes the neurons \quote{initial} confidence that the feature it is looking for is present in the input.
The operation which does both of these things is the {\em Hadamard product}.
The Hadamard product can be viewed as an extension of the dot product to two dimentional arrays.
It combines two arrays---of the same dimentions---by multiplying corresponding entries together and summing the results.
Which is precisly what the dot product does with two vectors.

The Hadamard product (${\rm Hp}$) of two matricies $A,B$ with entries $a_{i,j},b_{i,j}$ of equal dimentions is
\startplaceformula[reference=hdprod]
\startformula
{\rm Hp}(A,B) = \sum_{j} \sum_{i} a_{i,j} \cdot b_{i,j}
\stopformula
\stopplaceformula

\indentation
Another thing we need to take into account is the dimentions of our new weight array.
If we were to proceed analogically to how a multilayer percepetron works, the array should have the same size as the input to the neuron.
\quote{Connecting} each value in the input to an individual weight in the neuron.
But this approach means that each neuron, in principle, looks for a single feature in the entire input at once.
A more refined approach, is to let the neuron's weight array be smaller in size than its input.
Instead of applying the weight array (using the Hadamard product) to the entire input at once, we apply it (still using the Hadamard product) to portions of the input seperately.
Intuitivly, this means that the weight arrays are \quote{scanned} across the entire input image.
This allows the neuron to be trained to look for a single feature, such as a \quote{sharp edge} or a \quote{round corner}, in multiple areas of the input.
The result of this method will no longer be a single number representing how \quote{initialy} confident the neuron is that a particular feature is present in the input {\em\bf as a whole}.
Rather, the result becomes a {\em feature map}.
Another two dimentional array which representes how \quote{initialy} confident the neuron is that a particular feature is present in {\em\bf specific locations} of the input.

Going one step further, we can allow the neuron to treat inputs of not only a single two dimentional array, but several two dimentional arrays.
A typical example where the input would consists of several interlinked two dimentional arrays is RGB images.
An RGB image consists of three arrays of pixel values (numbers) that desribe how red, green and blue an image is in each pixel.
The number of interlinked two dimentional arrays present in the input, is known as the number of {\em channels} that the input has.
In order for our neuron to treat inputs with more than one channel we let the neuron have as many channels as the input.
That is to say, we equip the neuron with a weight array for each channel in the input.
For each channel the weight arrays are applied to the input (using Hadamard) and the result in each channel is combined to form a final single feature map.

A neural network which makes use of layers of neurons of this kind, is a convolutional neural network.
The multi-channel two dimentional arrays of weights inside each such neuron is called the neuron's {\em kernel} or {\em filter}.
Why are these neural networks called convolutional neural networks?
That will be explained in the next section.

\startsubsection[title=The convolutional neuron]
Let us start this section with a simple convolutional neuron.
The neuron's kernel consists of a single weight matrix (one channel), some bias and some activation function.
Consequently, the input that this neuron accepts is any one channel two dimentional array of size greater than its kernel.

The process of calcuating the neurons \quote{initial} confidence that the feature it is looking for is present in the input, is illustrated by Figure~\in[conv-operation] on page~\at[conv-operation].
Figure~\in[conv-operation] shows how the kernel \quote{scans} the entire input array to compute the feature map.
Let us construct the general formula for this feature map.

We will denote the feature map, kernel and input as $F$, $K$ and $M$ respectivly.
They are two dimentional arrays and their individual entries will be denoted as $F(x,y)$ where $x$ is the column index and $y$ the row index.
The indexes will all start at zero (e.g.\ $M(1,2)$ is the entry in the second column, third row of $M$).
There are many lengths involved in working with three seperate arrays, various lengths will here be denoted by $n$.
A subscript, either $x$ or $y$, will indicate if it is a horisontal or vertical length and a subsequent argument will indicate which array the length belongs to.
So for example, $n_x(M)$ is the horisontal length of the input array---the number of columns or x's in $M$.

If the input represents a typical MNIST image with dimentions $28{\rm x}28$, then $n_x(M) = 28$ and $n_y(M) = 28$.
In this scenario, the indices of $M$ range in $[0 \, .. \, n_x(M) - 1] \times [0 \, .. \, n_y(M) - 1]$.

Let us say that the kernel scans its input one column and one row at a time.
In this scenario, the output feature map will have dimentions
\stopsubsection

\startsubsection[title=The convolutional layer]
Okay this is where I will start developing the mathematics.
First we need to define the various variables used and some terminology.
A network has several layers and each layer has its own specific attributes.
A supersricpt encolsed in parantheses will be used as the layer index.


\startplaceformula[reference=dimen-feature-1]
\startformula
\startmathalignment[n=1]
\NC \delta x_Z^{(l+1)} = \delta x_W^{(l)} - \delta x_Z^{(l)} + 1 \NR
\NC y_Z^{(l+1)} = \delta y_W^{(l)} - \delta y_Z^{(l)} + 1 \NR
\stopmathalignment
\stopformula
\stopplaceformula

\startplaceformula
\startformula
\startmathalignment[n=2]
\NC \color[red]{x} \in [0 \, .. \, n_x(F) - 1] \qquad \NC \color[red]{y} \in [0 \, .. \, n_y(F) - 1] \NR
\NC \color[blue]{x} \in [0 \, .. \, n_x(K) - 1] \qquad \NC \color[blue]{y} \in [0 \, .. \, n_y(K) - 1] \NR
\stopmathalignment
\stopformula
\stopplaceformula

\indentation
The product in layer $l$ before activation, here denoted $\color[blue]{Z}$ is given by the following formula


Regular formula
\startplaceformula[reference=devel-feature-1]
\startformula
\color[green]{Z}_{\color[red]{x},\color[red]{y},\color[red]{c}}^{(l+1)}
=
\sum_{\color[blue]{c'} = 0}^{\delta c_Z^{(l)}}
\sum_{\color[blue]{y'} = 0}^{\delta y_Z^{(l)}}
\sum_{\color[blue]{x'} = 0}^{\delta x_Z^{(l)}}
\Bigl(
W_{\color[blue]{x'}, \color[blue]{y'}, \color[blue]{c'}}^{(l, \color[red]{c})}
Z_{\color[red]{x} + \color[blue]{x'}, \color[red]{y} + \color[blue]{y'}, \color[blue]{c'}}^{(l)}
\Bigr)
+
b^{(l,\color[red]{c})}
\stopformula
\stopplaceformula

Derivative with respect to $Z$.
$\color[magenta]{c^*} \in [\color[red]{c} \, .. \, \color[red]{c} + \color[magenta]{c^*}]$.
$\color[magenta]{y^*} \in [\color[red]{y} \, .. \, \color[red]{y} + \color[magenta]{y^*}]$.
$\color[magenta]{x^*} \in [\color[red]{x} \, .. \, \color[red]{x} + \color[magenta]{x^*}]$.

\startplaceformula
\startformula
\startmathalignment
\NC \frac
   {\partial \color[green]{Z}_{\color[red]{x},\color[red]{y},\color[red]{c}}^{(l+1)}}
   {\partial Z_{\color[magenta]{x^*}, \color[magenta]{y^*}, \color[magenta]{c^*}}^{(l)}}
= \NC
\frac
   {\partial}
   {\partial Z_{\color[magenta]{x^*}, \color[magenta]{y^*}, \color[magenta]{c^*}}^{(l)}}
\left(
\sum_{\color[blue]{c'} = 0}^{\delta c_W^{(l)}}
\sum_{\color[blue]{y'} = 0}^{\delta y_W^{(l)}}
\sum_{\color[blue]{x'} = 0}^{\delta x_W^{(l)}}
\Bigl(
W_{\color[blue]{x'}, \color[blue]{y'}, \color[blue]{c'}}^{(l, \color[red]{c})}
Z_{\color[red]{x} + \color[blue]{x'}, \color[red]{y} + \color[blue]{y'}, \color[blue]{c'}}^{(l)}
\Bigr)
+
b^{(l,\color[red]{c})}
\right) \NR
\NC = \NC
\sum_{\color[blue]{c'} = 0}^{\delta c_W^{(l)}}
\sum_{\color[blue]{y'} = 0}^{\delta y_W^{(l)}}
\sum_{\color[blue]{x'} = 0}^{\delta x_W^{(l)}}
\frac
   {\partial}
   {\partial Z_{\color[magenta]{x^*}, \color[magenta]{y^*}, \color[magenta]{c^*}}^{(l)}}
\left(
\Bigl(
W_{\color[blue]{x'}, \color[blue]{y'}, \color[blue]{c'}}^{(l, \color[red]{c})}
Z_{\color[red]{x} + \color[blue]{x'}, \color[red]{y} + \color[blue]{y'}, \color[blue]{c'}}^{(l)}
\Bigr)
+
b^{(l,\color[red]{c})}
\right) \NR
\NC = \NC
W_{\color[blue]{x'}, \color[blue]{y'}, \color[blue]{c'}}^{(l, \color[red]{c})}
\stopmathalignment
\stopformula
\stopplaceformula

Derivative with respecto to $W$.
$\color[magenta]{c^*} \in [0 \, .. \, \delta c_W^{(l)}]$.
$\color[magenta]{y^*} \in [0 \, .. \, \delta y_W^{(l)}]$.
$\color[magenta]{x^*} \in [0 \, .. \, \delta x_W^{(l)}]$.

\startplaceformula
\startformula
\startmathalignment
\NC \frac
   {\partial \color[green]{Z}_{\color[red]{x},\color[red]{y},\color[red]{c}}^{(l+1)}}
   {\partial W_{\color[magenta]{x^*}, \color[magenta]{y^*}, \color[magenta]{c^*}}^{(l, \color[red]{c})}}
= \NC
\frac
   {\partial}
   {\partial W_{\color[magenta]{x^*}, \color[magenta]{y^*}, \color[magenta]{c^*}}^{(l, \color[red]{c})}}
\left(
\sum_{\color[blue]{c'} = 0}^{\delta c_W^{(l)}}
\sum_{\color[blue]{y'} = 0}^{\delta y_W^{(l)}}
\sum_{\color[blue]{x'} = 0}^{\delta x_W^{(l)}}
\Bigl(
W_{\color[blue]{x'}, \color[blue]{y'}, \color[blue]{c'}}^{(l, \color[red]{c})}
Z_{\color[red]{x} + \color[blue]{x'}, \color[red]{y} + \color[blue]{y'}, \color[blue]{c'}}^{(l)}
\Bigr)
+
b^{(l,\color[red]{c})}
\right) \NR
\NC = \NC
\sum_{\color[blue]{c'} = 0}^{\delta c_W^{(l)}}
\sum_{\color[blue]{y'} = 0}^{\delta y_W^{(l)}}
\sum_{\color[blue]{x'} = 0}^{\delta x_W^{(l)}}
\frac
   {\partial}
   {\partial W_{\color[magenta]{x^*}, \color[magenta]{y^*}, \color[magenta]{c^*}}^{(l, \color[red]{c})}}
\left(
\Bigl(
W_{\color[blue]{x'}, \color[blue]{y'}, \color[blue]{c'}}^{(l, \color[red]{c})}
Z_{\color[red]{x} + \color[blue]{x'}, \color[red]{y} + \color[blue]{y'}, \color[blue]{c'}}^{(l)}
\Bigr)
+
b^{(l,\color[red]{c})}
\right) \NR
\NC = \NC
Z_{\color[magenta]{x^*} + \color[red]{x'}, \color[magenta]{y^*} + \color[red]{y'}, \color[magenta]{c^*}}^{(l)} \NR
\stopmathalignment
\stopformula
\stopplaceformula

Derivative with respect to $b$
\startplaceformula
\startformula
\startmathalignment
\NC \frac
   {\partial \color[green]{Z}_{\color[red]{x},\color[red]{y},\color[red]{c}}^{(l+1)}}
   {\partial b^{(l,\color[red]{c})}}
= \NC
\frac
   {\partial}
   {\partial b^{(l,\color[red]{c})}}
\left(
\sum_{\color[blue]{c'} = 0}^{\delta c_W^{(l)}}
\sum_{\color[blue]{y'} = 0}^{\delta y_W^{(l)}}
\sum_{\color[blue]{x'} = 0}^{\delta x_W^{(l)}}
\Bigl(
W_{\color[blue]{x'}, \color[blue]{y'}, \color[blue]{c'}}^{(l, \color[red]{c})}
Z_{\color[red]{x} + \color[blue]{x'}, \color[red]{y} + \color[blue]{y'}, \color[blue]{c'}}^{(l)}
\Bigr)
+
b^{(l,\color[red]{c})}
\right) \NR
\NC = \NC 1 \NR
\stopmathalignment
\stopformula
\stopplaceformula

If we add a bias $b$ to this filter, the formula becomes

\startplaceformula[reference=devel-feature-2]
\startformula
F_*(\color[red]{x},\color[red]{y})
= 
\sum_{\color[blue]{y}}^{} 
\sum_{\color[blue]{x}}^{} 
\Bigl(
K(\color[blue]{x},\color[blue]{y})
\cdot
M(\color[red]{x} + \color[blue]{x}, \color[red]{y} + \color[blue]{y})
+
b
\Bigr)
\stopformula
\stopplaceformula

Sending this formula throug an activation function such as ReLU or the sigmoid function, we obtain the final output feature.

\startplaceformula[reference=devel-feature-3]
\startformula
F_*(\color[red]{x},\color[red]{y})
= 
{\rm activation}
\left(
\sum_{\color[blue]{y}}^{} 
\sum_{\color[blue]{x}}^{} 
\Bigl(
K(\color[blue]{x},\color[blue]{y})
\cdot
M(\color[red]{x} + \color[blue]{x}, \color[red]{y} + \color[blue]{y})
+
b
\Bigr)
\right)
\stopformula
\stopplaceformula

\startplacefigure[reference=conv-operation,
                   title={The basic forward operation of a convolutional layer},
                   location=top]
\startcombination[3*3]
{\externalfigure[./Images/conv-0.jpg][width=.3\textwidth]}{}
{\externalfigure[./Images/conv-1.jpg][width=.3\textwidth]}{}
{\externalfigure[./Images/conv-2.jpg][width=.3\textwidth]}{}
{\externalfigure[./Images/conv-3.jpg][width=.3\textwidth]}{}
{\externalfigure[./Images/conv-4.jpg][width=.3\textwidth]}{}
{\externalfigure[./Images/conv-5.jpg][width=.3\textwidth]}{}
{\externalfigure[./Images/conv-6.jpg][width=.3\textwidth]}{}
{\externalfigure[./Images/conv-7.jpg][width=.3\textwidth]}{}
{\externalfigure[./Images/conv-8.jpg][width=.3\textwidth]}{}
\stopcombination
\stopplacefigure

\startsubsubsection[title=Where is the convolution?]
In mathematics, the convolution operation is denoted by $*$ and the convolution $s$ (itself a function) of two functions $k$ and $m$ is defined as

\startplaceformula[reference=cont-conv]
\startformula
s(t) = (k * m)(t) = \int_{{-}\infty}^{{+}\infty} k(x) \cdot m(t-x) \, {\rm d}x
\stopformula
\stopplaceformula
\startplaceformula[reference=disc-conv]
\startformula
s(\color[red]{x}) = (k * m)(\color[red]{x}) = \sum_{\color[blue]{x} = {-}\infty}^{{+}\infty} k(\color[blue]{x}) \cdot m(\color[red]{x}-\color[blue]{x})
\stopformula
\stopplaceformula

For continuous (Eq.~(\in[cont-conv])) and discrete (Eq.~(\in[disc-conv])) functions respectivly.
On a computer, we are in practice always working with discrete convolutions so for us the convolution of interest is Eq.~(\in[disc-conv]).
If we make the functions $k$ and $m$ two dimentional---they take two arguments as their input---their convolution also becomes two dimentional.

\startplaceformula[reference=twodim-disc-conv]
\startformula
s(\color[red]{x}, \color[red]{y}) = (k * m)(\color[red]{x}, \color[red]{y}) =
\sum_{\color[blue]{y} = {-}\infty}^{{+}\infty}
\sum_{\color[blue]{x} = {-}\infty}^{{+}\infty}
k(\color[blue]{x}, \color[blue]{y}) \cdot m(\color[red]{x}-\color[blue]{x}, \color[red]{y} - \color[blue]{y})
\stopformula
\stopplaceformula

\indentation
Let us known say that $k$ and $m$ are two functions which each index an array of two dimentions.
Meaning that the functions take two indexes as their input arguments.
Let us further assume that $k$ and $m$'s arrays are zero at any index but for those contained in a small area.
For $k$ that area is $[0 \, .. \, n_x(k) - 1] \times [0 \, .. \, n_y(k) - 1]$ and for $m$ it is $[0 \, .. \, n_x(m) - 1] \times [0 \, .. \, n_y(m) - 1]$.
In this case, the convolution of $k$ and $m$ reduces to

\startplaceformula[reference=twodim-disc-conv-finite]
\startformula
s(\color[red]{x}, \color[red]{y}) = (k * m)(\color[red]{x}, \color[red]{y}) = 
\sum_{\color[blue]{y} = 0}^{n_y(k) - 1} 
\sum_{\color[blue]{x} = 0}^{n_x(k) - 1} 
k(\color[blue]{x}, \color[blue]{y}) \cdot m(\color[red]{x}-\color[blue]{x}, \color[red]{y} - \color[blue]{y})
\stopformula
\stopplaceformula

Where $\color[red]{x} \in [0 \, .. \, n_x(m) - 1]$ and $\color[red]{y} \in [0 \, .. \, n_y(m) - 1]$.

The resemblance to Eq.~(\in[devel-feature-1]) is already apparent but there is a subtle difference.
Eq.~(\in[twodim-disc-conv-finite]) subtracts the blue indices from the red ones while Eq.~(\in[devel-feature-1]) adds them together.
The effect of subtracting instead of adding renders the convolution operation $(*)$ commutative.

\startplaceformula[reference=conv-commute]
\startformula
\startmathalignment
\NC (k * m)(\color[red]{x}, \color[red]{y}) \NC = (m * k)(\color[blue]{x}, \color[blue]{y}) \NR
\NC \sum_{\color[blue]{x}}^{} \sum_{\color[blue]{y}}^{} k(\color[blue]{x}, \color[blue]{y}) \cdot m(\color[red]{x}-\color[blue]{x}, \color[red]{y} - \color[blue]{y}) \NC
= \sum_{\color[red]{x}}^{} \sum_{\color[red]{y}}^{} m(\color[red]{x}, \color[red]{y}) \cdot k(\color[blue]{x}-\color[red]{x}, \color[blue]{y} - \color[red]{y}) \NR
\stopmathalignment
\stopformula
\stopplaceformula

\indentation
So what is the effect of replacing the pluses in Eq.~(\in[devel-feature-1]) with minuses?
Well at first glance it seems that you cannot do it as you would end up with indexes that are out of bounds for the input image $M$.
But let us assume that $M$ behaves as $m$ meaning that it is zero everywhere where the indices are out of bounds.
What you find if you visualize the computation of $F$ using Eq.~(\in[twodim-disc-conv-finite]), is that it is effectivly the same as using Eq.~(\in[devel-feature-1]) with the kernel flipped about its horisontal and vertical axes.
Which in turn means that flipping the kernel relative to the input, renders the computation of the feature map commutative.
But this is a property that is usefull in mathematical contexts and less so for computing neural networks.
It is Eq.~(\in[devel-feature-1]) that is commonly used in macine learning applications and the associated mathematical operations is actually what is called the {\em cross-correlation}.
Despite this, neural networks that implement these kinds of neurons are called convolutional even if the underlying mathematical operations is more often than not a close relative of the proper convolution operation.
\stopsubsubsection

\startsubsubsection[title=Multiple channels]
Let us return to Eq.~(\in[devel-feature-3]) for a minute and ask: what happens if we add some channels to the input?
Let us denote the number of channels by $n_c$.
The input and kernel are now multi-channel arrays, their individual entries will be denoted as $K(i,j,c)$ with $c$ ranging in $[0 \, .. \, n_c - 1]$
To calculate the feature map when multiple channels are present, we have to take into account the contribution of each individual channel.
We do this by simply adding them together.
Since we only want to add the bias of the filter once per entry in the feature map, the resulting modified formula becomes

\startplaceformula[reference=devel-feature-4]
\startformula
F(\color[red]{x},\color[red]{y})
=
{\rm activation}
\left( 
\sum_{\color[blue]{y}}^{}
\sum_{\color[blue]{x}}^{}
\left(
\sum_{c}^{}
\Bigl(
K(\color[blue]{x},\color[blue]{y},c)
\cdot
M(\color[red]{x} + \color[blue]{x}, \color[red]{y} + \color[blue]{y},c)
\right)
+
b
\right)
\stopformula
\stopplaceformula
\stopsubsubsection

\startsubsubsection[title=The stride]
So far we have considered a feature map produced by scanning the kernel over the input one {\em step} at a time.
As is demonstrated in Figure~\in[conv-operation] on page~\at[conv-operation].
However, we could let the kernel take longer steps instead of moving just one row or one column at a time.
The \quote{length} of each step that the kernel takes while moving over the input is what is known as the {\em stride}.
Which we can further divide into the horisontal stride $s_x$ and vertical stride $s_y$.

What purpose does altering the stride serve?
Increasing the stride reduces the size of the resulting feature map since it is computed at fewer areas.
Which has the net effect that the {\em computational complexity} of the neuron is reduced.
However, it also means that the neuron studies its input less which can result in vital information in the input being overlooked.
This is not a big problem as long as the features of the input is judged to be larger than the kernel that the neuron uses.
So long as this is the case, increasing the stride of a neuron can significantly reduce its computational complexity without resulting in it no longer surving its purpose.

Let us now further develop Eq.~(\in[devel-feature-4]) to take into account variable strides.
It is as simple as multiplying the red indices with the strides.

\startplaceformula[reference=devel-feature-5]
\startformula
F(\color[red]{x},\color[red]{y})
=
{\rm activation}
\left( 
\sum_{\color[blue]{y}}^{}
\sum_{\color[blue]{x}}^{}
\left(
\sum_{c}^{}
\Bigl(
K(\color[blue]{x},\color[blue]{y},c)
\cdot
M(\color[red]{x}s_x + \color[blue]{x}, \color[red]{y}s_y + \color[blue]{y},c)
\right)
+
b
\right)
\stopformula
\stopplaceformula
\stopsubsubsection

\startsubsubsection[title=Zero padding]
If we increase a neurons stride we can shrink the size of the resulting feature map.
But it does not allow us to increase the size of the feature map!
To do this, a techinque known as {\em zero padding} is used.
Instead of working with the input {\em as is}, one or more extra outer rows and columns of zeros are added onto it before the kernel is scanned across it.
This allows us to increase the size of the resulting feature map.

This techinque is predominantly used to make the feature map have the same dimentions as the input image in applicatons where this is desirable.
\stopsubsubsection



\startsubsection[title=The pooling \quote{neuron}]
{\em Downsampling} and purpose of downsampling.
Noise reduction and computational reduction.
\stopsubsection

\startsubsection[title=Backward Propagation]
Obtain formula for derivative of convolution and pooling.
\stopsubsection
\stopsection
