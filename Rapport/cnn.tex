\startsection[title={\em Convolutional} Neural Networks]
A convolutional neural network is an evolution of a classical multilayer percepetron network.
Recall the basic principle underpinning how a normal neuron in a neural network is supposed to work.
The neuron is supposed to \quote{look} for features in its input data.
If the neuron \quote{thinks} that those features are present in the input data it \quote{fires}.
Otherwise the neuron does not fire.

In a classical multilayer percepetron network this is implemented in the following way.
Each neuron contains a vector of weights, a bias and an activation function.
The input to the neuron---which must be a vector of equal length to the neuron's own weight vector---is combined with the neuron's weight vector using the dot product.
The neuron's bias is added onto the result which in turn is passed to the activation function which finally determines if the neuron \quote{fires} or not.

Using several layers of neurons one can achive quite remarkable results using this implementation of a neural network.
However, a multilayer percepetron is inherently limited.
The major problem is that neurons in these kinds of networks only accept input that is in the form of a vector.
This means that for applications where it is not natural for the input to be in a vector format, say image recognition, the input first has to be translated to a vector format.
Usually this results in a loss of information contained in the input.
In the typical case of image recongition, the input is in the form of one or more arrays of two dimentions.
For a multilayer percepetron to treat this input, the images has to be \quote{flattend} into a vector of one dimention before it can be passed on to the network.
This procedure eliminates some of the pixel relations in the image.
To deduce this, consider the process of reconstructing a flattend image.
If the image's dimentions prior to being flattend is not known, it is impossible, without the aid of pattern recognition, to reconstruct the image and be sure the reconstruction is equal to the original image.

To fix this problem, we can use a \quote{simple} solution.
Instead of having the neuron contain a vector of weights, let it have an array of weights.
If we change the neuron's vector of weights into an array, we also need to change the operation that is used to combine the weights with the input (which in a neuron of a multilayer percepetron is the dot product).
There are two things to consider here.
The purpose of the weights is to look for features or {\em patterns} in the input---by emphasizing or deemphasizing certain aspects---and the operation must reflect this purpose of the weights.
Furthermore, the result of the operation should be a single number which, in a sense, representes the neurons \quote{initial} confidence that the feature it is looking for is present in the input.
The operation which does both of these things is the {\em Hadamard product}.
The Hadamard product can be viewed as an extension of the dot product to two dimentional arrays.
It combines two arrays---of the same dimentions---by multiplying corresponding entries together and summing the results.
Which is precisly what the dot product does with two vectors.

The Hadamard product (${\rm Hp}$) of two matricies $A,B$ with entries $a_{i,j},b_{i,j}$ of equal dimentions is
\startplaceformula[reference=hdprod]
\startformula
{\rm Hp}(A,B) = \sum_{j} \sum_{i} a_{i,j} \cdot b_{i,j}
\stopformula
\stopplaceformula

\indentation
Another thing we need to take into account is the dimentions of our new weight array.
If we were to proceed analogically to how a multilayer percepetron works, the array should have the same size as the input to the neuron.
\quote{Connecting} each value in the input to an individual weight in the neuron.
But this approach means that each neuron, in principle, looks for a single feature in the entire input at once.
A more refined approach, is to let the neuron's weight array be smaller in size than its input.
Instead of applying the weight array (using the Hadamard product) to the entire input at once, we apply it (still using the Hadamard product) to portions of the input seperately.
Intuitivly, this means that the weight arrays are \quote{scanned} across the entire input image.
This allows the neuron to be trained to look for a single feature, such as a \quote{sharp edge} or a \quote{round corner}, in multiple areas of the input.
The result of this method will no longer be a single number representing how \quote{initialy} confident the neuron is that a particular feature is present in the input {\em\bf as a whole}.
Rather, the result becomes a {\em feature map}.
Another two dimentional array which representes how \quote{initialy} confident the neuron is that a particular feature is present in {\em\bf specific locations} of the input.

Going one step further, we can allow the neuron to treat inputs of not only a single two dimentional array, but several two dimentional arrays.
A typical example where the input would consists of several interlinked two dimentional arrays is RGB images.
An RGB image consists of three arrays of pixel values (numbers) that desribe how red, green and blue an image is in each pixel.
The number of interlinked two dimentional arrays present in the input, is known as the number of {\em channels} that the input has.
In order for our neuron to treat inputs with more than one channel we let the neuron have as many channels as the input.
That is to say, we equip the neuron with a weight array for each channel in the input.
For each channel the weight arrays are applied to the input (using Hadamard) and the result in each channel is combined to form a final single feature map.

A neural network which makes use of layers of neurons of this kind, is a convolutional neural network.
The multi-channel two dimentional arrays of weights inside each such neuron is called the neuron's {\em kernel} or {\em filter}.
Why are these neural networks called convolutional neural networks?
That will be explained in the next section.

\startsubsection[title=The convolutional neuron]
Let us start this section with a simple convolutional neuron.
The neuron's kernel consists of a single weight matrix (one channel), some bias and some activation function.
Consequently, the input that this neuron accepts is any one channel two dimentional array of size greater than its kernel.

The process of calcuating the neurons \quote{initial} confidence that the feature it is looking for is present in the input, is illustrated by Figure~\in[conv-operation] on page~\at[conv-operation].
Figure~\in[conv-operation] shows how the kernel \quote{scans} the entire input array to compute the feature map.
Let us construct the general formula for this feature map.

We will denote the feature map, kernel and input as $F$, $K$ and $M$ respectivly.
They are two dimentional arrays and their individual entries will be denoted as $F(i,j)$ where $i$ is the row index and $j$ the column index.
The indexes will all start at zero (e.g.\ $M(1,2)$ is the entry in the second row, third column of $M$. $M(0,0)$ is the {\em first} entry).
Horisontal and vertical lengths of arrays will be denoted by $h$ and $v$ with the associated array as a subscript (e.g.\ $h_K$ is the horisontal length of the kernel).
A typical MNIST image of dimentions $28{\rm x}28$ as the input, will have $h_M = v_M = 28$, its indexes will range in $[0 \, .. \, v_M - 1] \times [0 \, .. \, h_M - 1]$.

Let us say that the kernel scans its input one column or one row at a time.
In which case, the output feature map will have dimentions

\startplaceformula[reference=dimen-feature-1]
\startformula
h_F = h_M - h_K + 1
\stopformula
\startformula
v_F = v_M - v_K + 1
\stopformula
\stopplaceformula

The entries in the inital $F^*$ (before activation) are produced by applying the Hadamard product (Eq.~(\in[hdprod])) to each area of dimentions $h_K\,{\rm x}\,v_K$ in the input.
Thus the entry at row $\color[red]{i} \in [0 \, .. \, v_F - 1]$ and column $\color[red]{j} \in [0 \, .. \, h_F - 1]$ is computed as

\startplaceformula[reference=devel-feature-1]
\startformula
F^*(\color[red]{i},\color[red]{j}) = \sum_{\color[blue]{i} = 0}^{v_K - 1} \sum_{\color[blue]{j} = 0}^{h_K - 1} K(\color[blue]{i},\color[blue]{j}) \cdot M(\color[red]{i} + \color[blue]{i}, \color[red]{j} + \color[blue]{j})
\stopformula
\stopplaceformula

If we add a bias $b$ to this filter, the formula becomes

\startplaceformula[reference=devel-feature-2]
\startformula
F^*(\color[red]{i},\color[red]{j}) = \sum_{\color[blue]{i} = 0}^{v_K - 1} \sum_{\color[blue]{j} = 0}^{h_M - 1} \Bigl( K(\color[blue]{i},\color[blue]{j}) \cdot M(\color[red]{i} + \color[blue]{i}, \color[red]{j} + \color[blue]{j}) + b \Bigr)
\stopformula
\stopplaceformula

Lastly, the final output feature map $(F)$ of the filter is produced by sending all of these entries through an activation function (ReLU or sigmoid in practice).

\startplaceformula[reference=devel-feature-3]
\startformula
F(\color[red]{i},\color[red]{j}) = {\rm activation} \left( \sum_{\color[blue]{i} = 0}^{v_K - 1} \sum_{\color[blue]{j} = 0}^{h_M - 1} \Bigl( K(\color[blue]{i},\color[blue]{j}) \cdot M(\color[red]{i} + \color[blue]{i}, \color[red]{j} + \color[blue]{j}) + b \Bigr) \right)
\stopformula
\stopplaceformula

\startplacefigure[reference=conv-operation,
                   title={The basic forward operation of a convolutional layer},
                   location=top]
\startcombination[3*3]
{\externalfigure[./Images/conv-0.jpg][width=.3\textwidth]}{}
{\externalfigure[./Images/conv-1.jpg][width=.3\textwidth]}{}
{\externalfigure[./Images/conv-2.jpg][width=.3\textwidth]}{}
{\externalfigure[./Images/conv-3.jpg][width=.3\textwidth]}{}
{\externalfigure[./Images/conv-4.jpg][width=.3\textwidth]}{}
{\externalfigure[./Images/conv-5.jpg][width=.3\textwidth]}{}
{\externalfigure[./Images/conv-6.jpg][width=.3\textwidth]}{}
{\externalfigure[./Images/conv-7.jpg][width=.3\textwidth]}{}
{\externalfigure[./Images/conv-8.jpg][width=.3\textwidth]}{}
\stopcombination
\stopplacefigure

\startsubsubsection[title=Where is the convolution?]
In mathematics, the convolution operation is denoted by $*$ and the convolution $s$ (itself a function) of two functions $k$ and $m$ is defined as

\startplaceformula[reference=cont-conv]
\startformula
s(t) = (k * m)(t) = \int_{{-}\infty}^{{+}\infty} k(x) \cdot m(t-x) \, {\rm d}x
\stopformula
\stopplaceformula
\startplaceformula[reference=disc-conv]
\startformula
s(\color[red]{i}) = (k * m)(\color[red]{i}) = \sum_{\color[blue]{i} = {-}\infty}^{{+}\infty} k(\color[blue]{i}) \cdot m(\color[red]{i}-\color[blue]{i})
\stopformula
\stopplaceformula

For continuous (Eq.~(\in[cont-conv])) and discrete (Eq.~(\in[disc-conv])) functions respectivly.
On a computer, we are in practice always working with discrete convolutions so for us the convolution of interest is Eq.~(\in[disc-conv]).
If we make the functions $k$ and $m$ two dimentional---they take two arguments as an input---their convolution also becomes two dimentional.

\startplaceformula[reference=twodim-disc-conv]
\startformula
s(\color[red]{i}, \color[red]{j}) = (k * m)(\color[red]{i}, \color[red]{j}) = \sum_{\color[blue]{i} = {-}\infty}^{{+}\infty} \sum_{\color[blue]{j} = {-}\infty}^{{+}\infty} k(\color[blue]{i}, \color[blue]{j}) \cdot m(\color[red]{i}-\color[blue]{i}, \color[red]{j} - \color[blue]{j})
\stopformula
\stopplaceformula

\indentation
Let us known say that $k$ and $m$ are two functions which index each an array of two dimentions.
Meaning that the functions take two indexes as their input arguments.
Let us further assume that $k$ and $m$'s arrays are zero at any index but for those contained in a small area.
For $k$ that area is $[0 \, .. \, v_k - 1] \times [0 \, .. \, h_k - 1]$ and for $m$ it is $[0 \, .. \, v_m - 1] \times [0 \, .. \, h_m - 1]$.
Where $h$ (horisontal length) and $v$ (vertical length)---of both subscripts---are natural non-zero numbers.
In this case, the convolution of $k$ and $m$ reduces to

\startplaceformula[reference=twodim-disc-conv-finite]
\startformula
s(\color[red]{i}, \color[red]{j}) = (k * m)(\color[red]{i}, \color[red]{j}) = \sum_{\color[blue]{i} = 0}^{v_k - 1} \sum_{\color[blue]{j} = 0}^{h_k - 1} k(\color[blue]{i}, \color[blue]{j}) \cdot m(\color[red]{i}-\color[blue]{i}, \color[red]{j} - \color[blue]{j})
\stopformula
\stopplaceformula

Where $\color[red]{i} \in [0 \, .. \, v_m - 1]$ and $\color[red]{j} \in [0 \, .. \, h_m - 1]$.

The resemblance to Eq.~(\in[devel-feature-1]) is already apparent but there is a subtle difference.
Eq.~(\in[twodim-disc-conv-finite]) subtracts the blue indices from the red ones while Eq.~(\in[devel-feature-1]) adds them together.
The effect of subtracting instead of adding renders the convolution operation $*$ commutative.

\startplaceformula[reference=conv-commute]
\startformula
\startmathalignment
\NC (k * m)(\color[red]{i}, \color[red]{j}) \NC = (m * k)(\color[blue]{i}, \color[blue]{j}) \NR
\NC \sum_{\color[blue]{i} = 0}^{v_k - 1} \sum_{\color[blue]{j} = 0}^{h_k - 1} k(\color[blue]{i}, \color[blue]{j}) \cdot m(\color[red]{i}-\color[blue]{i}, \color[red]{j} - \color[blue]{j}) \NC
= \sum_{\color[red]{i} = 0}^{v_m - 1} \sum_{\color[red]{j} = 0}^{h_m - 1} m(\color[red]{i}, \color[red]{j}) \cdot k(\color[blue]{i}-\color[red]{i}, \color[blue]{j} - \color[red]{j}) \NR
\stopmathalignment
\stopformula
\stopplaceformula

\indentation
Bringing the discussion back to neural networks, kernels and inputs---what is the effect of replacing the pluses in Eq.~(\in[devel-feature-1]) with minuses?
Well at first glance it seems that you cannot do it as you would end up with indexes that are out of bounds for the input image $M$.
But let us assume that $M$ behaves as $m$ meaning that it is zero everywhere where the indices are out of bounds.
Suppose we have a kernel that has dimentions $h_K = v_K = 3$ and an input with dmientions $h_M = v_M = 5$, as in Figure~\in[conv-operation].
Suppose we are computing $F$ at position $i = 5, j = 5$ i.e.\ $F(5,5)$.
Notice how the top-left coordinate of $K$ pairs up with the bottom-right coordiante of the covered region of $M$
\startformula
K(0,0) \cdot M(5 - 0,5 - 0) = K(0,0) \cdot M(5,5)
\stopformula
And the top-right coordiante of $K$ pairs up with the bottom-left coordiante of the covered region of $M$
\startformula
K(0,2) \cdot M(5 - 0,5 - 2) = K(0,0) \cdot M(5,3)
\stopformula
Likewise, the bottom-left pairs with the top-right and the bottom-right pairs with the top-left.
\startformula
K(2,0) \cdot M(5 - 2,5 - 0) = K(2,0) \cdot M(3,5)
\stopformula
\startformula
K(2,2) \cdot M(5 - 2,5 - 2) = K(0,0) \cdot M(3,3)
\stopformula

\indentation
It appears that what you end with, is a kernel {\em flipped} about its horisontal and vertical axes.
This means that flipping the kernel relative to the input, renders the computation of the feature map commutative.
But this is a property that is usefull in mathematical contexts and less so for neural networks.
It is Eq.~(\in[devel-feature-1]) that is commonly used in macine learning applications and the associated mathematical operations is actually what is called the {\em cross-correlation}.
Despite this, neural networks that implement these kinds of neurons are called convolutional even if the underlying mathematical operations is more often than not a close relative of the proper convolution operation.
\stopsubsubsection

\startsubsubsection[title=Multiple channels]
Let us now add some channels to this neuron.
We will denote the number of channels by $nC$.
The input and kernel are now multi-channel arrays, their individual entries will be denoted as $K(i,j,c)$.
We have to add the contribution of each channel to the final feature map but we only add the bias once.
Which means that we end up with the following formula

\startplaceformula
\startformula
F(i',j') = {\rm activation} \left( \sum_{i = 1}^{v_K} \sum_{j = 1}^{hM} \left( \sum_{c = 1}^{nC} \Bigl( K(i,j,c) \cdot M(i' + i, j + j',c) \right) + b \right)
\stopformula
\stopplaceformula
\stopsubsubsection

\startsubsubsection[title=The stride]
What the stride is and the effects of altering it.
\stopsubsubsection

\startsubsubsection[title=Zero padding]
\stopsubsubsection



\startsubsection[title=The pooling \quote{neuron}]
{\em Downsampling} and purpose of downsampling.
Noise reduction and computational reduction.
\stopsubsection

\startsubsection[title=Backward Propagation]
Obtain formula for derivative of convolution and pooling.
\stopsubsection
\stopsection