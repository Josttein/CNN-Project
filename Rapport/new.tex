\startsection[title=Convolutional neural networks and their components]
Let us start this section with a {\bf brief} recall of what a neural network {\em is}.
A neural network is, intuitivly, a \quote{network} of \quote{neurons} which are connected together in a layerd structure.
Each layer contains several neurons that are each connected to all the neurons of the previous layer. 
The neurons are supposed to \quote{look} for certain \quote{features} in its input data.
If the neuron \quote{thinks} that these features are present it \quote{fires}.
If it does not think that the input data has the features it is looking for, the neuron does not fire.
In addition to layers of neurons, nerual networks can also incorporate special \quote{non-thinking} layers.
Layers in which there are no neurons.

A neural network's job is to process some input data and produce a final \quote{ativation}.
This final activation represents what the nerural network \quote{thinks} about the input it has processed.
A very common application of nerual networks sees them used to classify large data sets.
In this context, the output of the network is often interpreted as a probability which represents how confident the network is that one piece of data belongs to a particular class.
The convolutional neural network that we are going to be using as an example in this paper, serves such a classification purpose.

In this section we are going to develop the equations which govern the operation of each component or layer in a convolutional neural network.

\startsubsubject[title=Terminology and notation]
Before we delve in to the components of a convolutional neural network, we need to introduce some terms and notation.
Scalars will be represented with a lowercase letter (e.g.\ $x$).
Vectors will be represented with a bold lowercase letter (e.g.\ $\bi x$).
Matricies will be represented with an uppercase letter (e.g.\ $X$).
Vectors of matricies---vectors where each entry is a matrix---will be represented with a bold uppercase letter (e.g.\ $\bi X$).
This scheme has the added benefit that for the most part, aggregates of a layer will be represented in bold while data in individual channels will be represented in normal style letters.

Speaking of layers we need some symbols for the various parts that make up a layer.
Firstly we will need symbols for the two principle components of a neuron.
Namely its weights and its bias.
Weights will be represented with the letter \quote{w} while the bias will be represented with the letter \quote{b}.
The preliminary output of a neruon (or layer of neurons), before it is passed through an activation function, will be represented with the letter \quote{z}.
The output after activation will be represented with the letter \quote{a}.
Where does one layer end and the next one begin?
Everything after the previous layer's activation up until and including the current layer's activation constitues the layer in question.

We need the ability to reference specific positions in each layer and for that we need {\em a lot} of indexes.
Since we want our mathematics to match our code, to a certain extent, all the indices used in this paper will start at $0$.
The layer index which tells us which layer we are looking at will be written using a parenthetical superscript (e.g.\ ${\bi a}^{(l)}$ is the activation vector of layer $l$).
The channel index which tells us which channel we are looking at will be written using a parenthetical subscript (e.g.\ $a^{(l)}_{(c)}$ is the scalar activation of layer $l$ in channel $c$).
The output of each neuron in a layer becomes a channel in the next layer.
To reference both the input and output channel, a double parenthetical subscript will be used (e.g.\ $w^{(l)}_{(c,c')}$ is the scalar weight in layer $l$ connecting channel $c$ from layer $l$ to channel $c'$ in layer $l-1$).
To reference positions within a matrix we will use regular xy-coordinates (e.g.\ $a(x,y)$ is the scalar in row $x$ at column $y$).
Equations which relate a previous layer to the next one will generally use blue color to makr indicies belonging to the previous layer.
Red color will be used to mark indices beloning to the next layer.

We will often need to know the {\em dimensions} of layer.
How many channels does it have and what are the sizes of the matrices in that layer.
These dimensions will be written using the greek letter eta ($\eta$) with a layer index and a subscript to identify which dimention is expressed.
A \quote{c} subscript means it is the number of channels in the layer {\em minus one}.
A \quote{x} and \quote{y} subscript means it is row and column length {\em minus one} respectivly.
We use the letter $\eta$ instead of $n$ to remind the reader that all dimensions are substracted 1, because we want all our indices to start at zero.
As an example, if layer $3$ has eight individual channels then $\eta^{(3)}_c = 7$.
In addition to the $\eta$ dimentions we are going to need the dimentions of various weight matricies (kernels).
These will use the same kind of notation but with $k$ instead of $\eta$.

The inital input to a network can be though of as the first activation to the network.
So we are going to denote the inital input using the appropriate \quote{a} letter with a zero in parenthetical subscript.
\stopsubsubject


\startsubsection[title=Fully-connected layer]
\input 2/fc-layer.tex
\stopsubsection

\startsubsection[title=Convolutional layer]
\input 2/cv-layer.tex
\stopsubsection

\startsubsection[title=Downsampling layer]
\input 2/dv-layer.tex
\stopsubsection

\startsubsection[title=The activation function]
\input 2/activ.tex
\stopsubsection

\startsubsubsection[title=Output layer]
Descriptive text \ldots

Note that the last layer before the output layer does not need an activation function.

The equations are
\startplaceformula[reference=ot:forward]
\startformula
\startmathalignment
\NC \tilde{y}_{(\color[red]{c})} \NC =
\frac
    {
        \exp 
        \left(
            a^{(l)}_{(\color[red]{c})} 
        \right)
    }{
        \sum_{\color[red]{c^*} = 0}^{\eta^{(l)}_c}
        \exp 
        \left( 
            a^{(l)}_{(\color[red]{c^*})}
        \right)
    }
\NR[+]
\NC \ell \NC = 
- \sum_{\color[red]{c} = 0}^{\eta^{(l)}_c} 
y_{(\color[red]{c})}
\log \left(
    \tilde{y}_{(\color[red]{c})}
\right)
\NR[+]
\stopmathalignment
\stopformula
\stopplaceformula
\stopsubsubsection

\startsubsection[title=Backward propagation]

\startsubsubsection[title=Output layer]
Descriptive text \ldots

\startformula
\frac
    {
        \partial L
    }{
        \partial \tilde{y}_{(\color[red]{c})}
    } =
\frac
    {
        \partial
    }{
        \partial \tilde{y}_{(\color[red]{c})}
    }
\left(
    - \sum_{\color[red]{c^*} = 0}^{\eta^{(l)}_c} 
    y_{(\color[red]{c^*})}
    \log \left(
        \tilde{y}_{(\color[red]{c^*})}
    \right)
\right) =
- \frac
    {
        y_{(\color[red]{c})}
    }{
        \tilde{y}_{(\color[red]{c})}
    }
\stopformula

\startformula
\startmathalignment
\NC \frac
    {
        \partial \tilde{y}_{(\color[red]{c})}
    }{
        \partial a^{(l)}_{(\color[red]{c})}
    } 
\NC =
\frac
    {
        \partial
    }{
        \partial a^{(l)}_{(\color[red]{c})}
    }
\left(
\frac
    {
        \exp
        \left(
            a^{(l)}_{(\color[red]{c})}
        \right)
    }{
        \sum_{\color[red]{c^*} = 0}^{\eta^{(l)}_c}
        \exp 
        \left( 
            a^{(l)}_{(\color[red]{c^*})}
        \right)
    }
\right)
\NR 
\NC \NC =
\frac
    {
        \exp
        \left(
            a^{(l)}_{(\color[red]{c})}
        \right)
    }{
        \sum_{\color[red]{c^*} = 0}^{\eta^{(l)}_c}
        \exp 
        \left( 
            a^{(l)}_{(\color[red]{c^*})}
        \right)
    }
- 
\left(
    \frac
        {
            \exp
            \left(
                a^{(l)}_{(\color[red]{c})}
            \right)
        }{
            \sum_{\color[red]{c^*} = 0}^{\eta^{(l)}_c}
            \exp 
            \left( 
                a^{(l)}_{(\color[red]{c^*})}
            \right)
        }
\right)^2
\NR
\NC \NC =
\tilde{y}_{(\color[red]{c})} -
\left(
    \tilde{y}_{(\color[red]{c})}
\right)^2
\NR
\NC \NC =
\tilde{y}_{(\color[red]{c})} (1 - \tilde{y}_{(\color[red]{c})})
\NR
\stopmathalignment
\stopformula

\startplaceformula[reference=ot:backward:operation]
\startformula
\frac
    {
        \partial L
    }{
        \partial a^{(l)}_{(\color[red]{c})}
    }
=
\frac
    {
        \partial L
    }{
        \partial \tilde{y}_{(\color[red]{c})}
    }
\frac
    {
        \partial \tilde{y}_{(\color[red]{c})}
    }{
        \partial a^{(l)}_{(\color[red]{c})}
    }
=
- \frac
    {
        y_{(\color[red]{c})}
    }{
        \tilde{y}_{(\color[red]{c})}
    }
\tilde{y}_{(\color[red]{c})} (1 - \tilde{y}_{(\color[red]{c})})
=
\tilde{y}_{(\color[red]{c})} - y_{(\color[red]{c})}
\stopformula
\stopplaceformula
\stopsubsubsection

\startsubsubsection[title=Fully-connected layer]
Descriptive text \ldots

\startformula
\frac
    {
        \partial z^{(l)}_{(\color[red]{c})}
    }{
        \partial a^{(l-1)}_{(\color[magenta]{c^*})}
    }
= 
\frac
    {
        \partial
    }{
        \partial a^{(l-1)}_{(\color[magenta]{c^*})}
    }
\left(
    \sum_{\color[blue]{c'} = 0}^{\eta^{(l-1)}_c}
    \Bigl( 
            w^{(l)}_{(\color[red]{c}, \color[blue]{c'})}
            \, a^{(l-1)}_{(\color[blue]{c'})}
    \Bigr) +
    b^{(l)}_{(\color[red]{c})}
\right)
=
w^{(l)}_{(\color[red]{c}, \color[magenta]{c^*})}
\stopformula
\stopsubsubsection

\startsubsubsection[title=Pooling]
Descriptive text \ldots
\stopsubsubsection

\startsubsubsection[title=Convolutional layer]
Descriptive text \ldots
\stopsubsubsection

\stopsubsection

\startsubsection[title=Fully-connected layer]
Admit that the choice of index is a bit odd but that it will make sense later.
Regular formula.
\startplaceformula[reference=formula-mlp-base]
\startformula
Z^{(l+1)}_{\color[red]{c}} = 
\sum_{\color[blue]{c'} = 0}^{\eta_c^{(l)}}
\Bigl(
W_{\color[blue]{c'}}^{(l, \color[red]{c})} 
Z_{\color[blue]{c'}}^{(l)}
\Bigr) + 
b^{(l, \color[red]{c})}
\stopformula
\stopplaceformula

Partial derivative with respect to a $Z^{(l)}$.
\startplaceformula
\startformula
\startmathalignment
\NC \frac
    {\partial Z^{(l+1)}_{\color[red]{c}}}
    {\partial Z^{(l)}_{\color[magenta]{c^*}}} \NC =
\frac
    {\partial}
    {\partial Z^{(l)}_{\color[magenta]{c^*}}} 
\left(
\sum_{\color[blue]{c'} = 0}^{\eta_c^{(l)}}
\Bigl(
W_{\color[blue]{c'}}^{(l, \color[red]{c})} 
Z_{\color[blue]{c'}}^{(l)}
\Bigr) + 
b^{(l, \color[red]{c})}
\right) \NR
\NC \NC = 
\sum_{\color[blue]{c'} = 0}^{\eta_c^{(l)}}
\left(
\frac
    {\partial}
    {\partial Z^{(l)}_{\color[magenta]{c^*}}} 
\Bigl(
W_{\color[blue]{c'}}^{(l, \color[red]{c})}
Z_{\color[blue]{c'}}^{(l)}
\Bigr)
\right) + 
0 \NR
\NC \NC =
W_{\color[magenta]{c^*}}^{(l, \color[red]{c})}
\stopmathalignment
\stopformula
\stopplaceformula

Partial derivative with respect to a $W^{(l, \color[red]{c})}$.
\startplaceformula
\startformula
\startmathalignment
\NC \frac
    {\partial Z^{(l+1)}_{\color[red]{c}}}
    {\partial W^{(l, \color[red]{c})}_{\color[magenta]{c^*}}} \NC =
\frac
    {\partial}
    {\partial W^{(l, \color[red]{c})}_{\color[magenta]{c^*}}} 
\left(
\sum_{\color[blue]{c'} = 0}^{\eta_c^{(l)}}
\Bigl(
W_{\color[blue]{c'}}^{(l, \color[red]{c})} 
Z_{\color[blue]{c'}}^{(l)}
\Bigr) + 
b^{(l, \color[red]{c})}
\right) \NR
\NC \NC = 
\sum_{\color[blue]{c'} = 0}^{\eta_c^{(l)}}
\left(
\frac
    {\partial}
    {\partial W^{(l, \color[red]{c})}_{\color[magenta]{c^*}}} 
\Bigl(
W_{\color[blue]{c'}}^{(l, \color[red]{c})}
Z_{\color[blue]{c'}}^{(l)}
\Bigr)
\right) + 
0 \NR
\NC \NC =
Z_{\color[magenta]{c^*}}^{(l)}
\stopmathalignment
\stopformula
\stopplaceformula

Partial derivative with respect to the bias $b^{(l, \color[red]{c})}$.
\startplaceformula
\startformula
\startmathalignment
\NC \frac
    {\partial Z^{(l+1)}_{\color[red]{c}}}
    {\partial b^{(l, \color[red]{c})}} \NC =
\frac
    {\partial}
    {\partial b^{(l, \color[red]{c})}} 
\left(
\sum_{\color[blue]{c'} = 0}^{\eta_c^{(l)}}
\Bigl(
W_{\color[blue]{c'}}^{(l, \color[red]{c})} 
Z_{\color[blue]{c'}}^{(l)}
\Bigr) + 
b^{(l, \color[red]{c})}
\right) \NR
\NC \NC = 
\sum_{\color[blue]{c'} = 0}^{\eta_c^{(l)}}
\left(
\frac
    {\partial}
    {\partial b^{(l, \color[red]{c})}} 
\Bigl(
W_{\color[blue]{c'}}^{(l, \color[red]{c})}
Z_{\color[blue]{c'}}^{(l)}
\Bigr)
\right) + 
1 \NR
\NC \NC =
1
\stopmathalignment
\stopformula
\stopplaceformula

\startsubsubsection[title=Layer level]
Now let us move on up to the layer level.
$Z$ without a subscript represents a column vector of all the entries.
\startformula
Z^{(l)} =
\startmatrix[left={\left(}, right={\right)}]
\NC Z^{(l)}_1 \NR
\NC Z^{(l)}_2 \NR
\NC \vdots \NR
\NC Z^{(l)}_{\eta_c^{(l)}} \NR
\stopmatrix
\stopformula
$W$ without a neuron index and without a subscript represents a matrix.
The neruon index selects row and the channel index selects column.
\startformula
W^{(l)} =
\startmatrix[left={\left(}, right={\right)}]
\NC W^{(l, 1)}_1 \NC W^{(l, 1)}_2 \NC \dots \NC W^{(l, 1)}_{\eta_c^{(l)}} \NR
\NC W^{(l, 2)}_1 \NC W^{(l, 2)}_2 \NC \dots \NC W^{(l, 2)}_{\eta_c^{(l)}} \NR
\NC \vdots \NC \vdots \NC \ddots \NC \vdots \NR
\NC W^{(l, \eta_c^{(l+1)})}_1 \NC W^{(l, \eta_c^{(l+1)})}_2 \NC \dots \NC W^{(l, \eta_c^{(l+1)})}_{\eta_c^{(l)}} \NR
\stopmatrix
\stopformula
$b$ without a neuron index is a column vector of biases.
\startformula
b^{(l)} =
\startmatrix[left={\left(}, right={\right)}]
\NC b^{(l,1)} \NR
\NC b^{(l,2)} \NR
\NC \vdots \NR
\NC b^{(l,\eta_c^{(l)})} \NR
\stopmatrix
\stopformula

Regular formula.
\startplaceformula
\startformula
Z^{(l+1)} = W^{(l)} \cdot Z^{(l)} + b^{(l)}
\stopformula
\stopplaceformula

Gradient with respect to a $Z^{(l)}$.
Hvordan skrive gradient med hensyn p√• noe?
\startplaceformula
\startformula
\frac
    {\partial Z^{(l+1)}}
    {\partial Z^{(l)}} = W^{(l)}
\stopformula
\stopplaceformula

Gradient with respect to a $W^{(l)}$.
\startplaceformula
\startformula
\frac
    {\partial Z^{(l+1)}}
    {\partial W^{(l)}} = Z^{(l)}
\stopformula
\stopplaceformula

Gradient with respect to a $b^{(l, \color[red]{c})}$.
\startplaceformula
\startformula
\frac
    {\partial Z^{(l+1)}}
    {\partial b^{(l)}} =
\startmatrix[left={\left(},right={\right)}]
\NC 1 \NR
\NC 1 \NR
\NC \vdots \NR
\NC 1 \NR
\stopmatrix
\stopformula
\stopplaceformula
\stopsubsubsection
\stopsubsection


\startsubsection[title=Convolutional layer]
Introduce the indexes.
$y$ selects row while $x$ selects column.
Regular formula.
\startplaceformula[reference=formula-conv-base]
\startformula
Z_{\color[red]{c},\color[red]{y},\color[red]{x}}^{(l+1)}
=
\sum_{\color[blue]{c'} = 0}^{\eta_c^{(l)}}
\sum_{\color[blue]{y'} = 0}^{\eta_y^{(l)}}
\sum_{\color[blue]{x'} = 0}^{\eta_x^{(l)}}
\Bigl(
W_{\color[blue]{c'}, \color[blue]{y'}, \color[blue]{x'}}^{(l, \color[red]{c})}
Z_{\color[blue]{c'}, \color[red]{y} + \color[blue]{y'}, \color[red]{x} + \color[blue]{x'}}^{(l)}
\Bigr)
+
b^{(l,\color[red]{c})}
\stopformula
\stopplaceformula

Partial derivative with respect to a $Z_{(l)}$.
$\color[magenta]{c^*} \in [\color[red]{c} \, .. \, \color[red]{c} + \eta_c^{(l)}]$.
$\color[magenta]{y^*} \in [\color[red]{y} \, .. \, \color[red]{y} + \eta_y^{(l)}]$.
$\color[magenta]{x^*} \in [\color[red]{x} \, .. \, \color[red]{x} + \eta_x^{(l)}]$.

\startplaceformula
\startformula
\startmathalignment
\NC \frac
   {\partial Z_{\color[red]{c},\color[red]{y},\color[red]{x}}^{(l+1)}}
   {\partial Z_{\color[magenta]{c^*}, \color[magenta]{y^*}, \color[magenta]{x^*}}^{(l)}}
\NC =
\frac
   {\partial}
   {\partial Z_{\color[magenta]{c^*}, \color[magenta]{y^*}, \color[magenta]{x^*}}^{(l)}}
\left(
\sum_{\color[blue]{c'} = 0}^{\eta_c^{(l)}}
\sum_{\color[blue]{y'} = 0}^{\eta_y^{(l)}}
\sum_{\color[blue]{x'} = 0}^{\eta_x^{(l)}}
\Bigl(
W_{\color[blue]{c'}, \color[blue]{y'}, \color[blue]{x'}}^{(l, \color[red]{c})}
Z_{\color[blue]{c'}, \color[red]{y} + \color[blue]{y'}, \color[red]{x} + \color[blue]{x'}}^{(l)}
\Bigr)
+
b^{(l,\color[red]{c})}
\right) \NR
\NC \NC =
\sum_{\color[blue]{c'} = 0}^{\eta_c^{(l)}}
\sum_{\color[blue]{y'} = 0}^{\eta_y^{(l)}}
\sum_{\color[blue]{x'} = 0}^{\eta_x^{(l)}}
\left(
\frac
   {\partial}
   {\partial Z_{\color[magenta]{c^*}, \color[magenta]{y^*}, \color[magenta]{x^*}}^{(l)}}
\Bigl(
W_{\color[blue]{x'}, \color[blue]{y'}, \color[blue]{c'}}^{(l, \color[red]{c})}
Z_{\color[red]{x} + \color[blue]{x'}, \color[red]{y} + \color[blue]{y'}, \color[blue]{c'}}^{(l)}
\Bigr)
\right)
+
0 \NR
\NC \NC =
W_{\color[magenta]{c^*}, \color[magenta]{y'}, \color[magenta]{x'}}^{(l, \color[red]{c})}
\stopmathalignment
\stopformula
\stopplaceformula

Partial derivative with respecto to a $W^{(l, \color[red]{c})}$.
$\color[magenta]{c^*} \in [0 \, .. \, \eta_c^{(l)}]$.
$\color[magenta]{y^*} \in [0 \, .. \, \eta_y^{(l)}]$.
$\color[magenta]{x^*} \in [0 \, .. \, \eta_x^{(l)}]$.

\startplaceformula
\startformula
\startmathalignment
\NC \frac
   {\partial Z_{\color[red]{c},\color[red]{y},\color[red]{x}}^{(l+1)}}
   {\partial W_{\color[magenta]{c^*}, \color[magenta]{y^*}, \color[magenta]{x^*}}^{(l, \color[red]{c})}}
\NC =
\frac
   {\partial}
   {\partial W_{\color[magenta]{c^*}, \color[magenta]{y^*}, \color[magenta]{x^*}}^{(l, \color[red]{c})}}
\left(
\sum_{\color[blue]{c'} = 0}^{\eta_c^{(l)}}
\sum_{\color[blue]{y'} = 0}^{\eta_y^{(l)}}
\sum_{\color[blue]{x'} = 0}^{\eta_x^{(l)}}
\Bigl(
W_{\color[blue]{c'}, \color[blue]{y'}, \color[blue]{x'}}^{(l, \color[red]{c})}
Z_{\color[blue]{c}, \color[red]{y} + \color[blue]{y'}, \color[red]{x} + \color[blue]{x'}}^{(l)}
\Bigr)
+
b^{(l,\color[red]{c})}
\right) \NR
\NC \NC =
\sum_{\color[blue]{c'} = 0}^{\eta_c^{(l)}}
\sum_{\color[blue]{y'} = 0}^{\eta_y^{(l)}}
\sum_{\color[blue]{x'} = 0}^{\eta_x^{(l)}}
\left(
\frac
   {\partial}
   {\partial W_{\color[magenta]{c^*}, \color[magenta]{y^*}, \color[magenta]{x^*}}^{(l, \color[red]{c})}}
\Bigl(
W_{\color[blue]{c'}, \color[blue]{y'}, \color[blue]{x'}}^{(l, \color[red]{c})}
Z_{\color[blue]{c}, \color[red]{y} + \color[blue]{y'}, \color[red]{x} + \color[blue]{x'}}^{(l)}
\Bigr)
\right)
+
0 \NR
\NC \NC = 
Z_{\color[magenta]{c^*}, \color[red]{y} + \color[magenta]{y^*}, \color[red]{x} + \color[magenta]{x^*}}^{(l)} \NR
\stopmathalignment
\stopformula
\stopplaceformula

Partial derivative with respect to $b^{l, \color[red]{c}}$.
\startplaceformula
\startformula
\startmathalignment
\NC \frac
   {\partial Z_{\color[red]{c},\color[red]{y},\color[red]{x}}^{(l+1)}}
   {\partial b^{(l,\color[red]{c})}}
\NC =
\frac
   {\partial}
   {\partial b^{(l,\color[red]{c})}}
\left(
\sum_{\color[blue]{c'} = 0}^{\eta_c^{(l)}}
\sum_{\color[blue]{y'} = 0}^{\eta_y^{(l)}}
\sum_{\color[blue]{x'} = 0}^{\eta_x^{(l)}}
\Bigl(
W_{\color[blue]{c'}, \color[blue]{y'}, \color[blue]{x'}}^{(l, \color[red]{c})}
Z_{\color[blue]{c}, \color[red]{y} + \color[blue]{y'}, \color[red]{x} + \color[blue]{x'}}^{(l)}
\Bigr)
+
b^{(l,\color[red]{c})}
\right) \NR
\NC \NC = 1 \NR
\stopmathalignment
\stopformula
\stopplaceformula

\startsubsubsection[title=Layer level]
Now let us move up to the layer level.
To progress up to the layer level we only need to deal with the hadamard product.
After that everything is exactly the same as for the fully-connected layer.
$Z_{\color[red]{c}}^{(l)}$ represents a two dimensional matrix.
\startformula
Z_{\color[red]{c}}^{(l)} = 
\startmatrix[left={\left(}, right={\right)}]
\NC Z_{\color[red]{c}, 1, 1}^{(l)} \NC Z_{\color[red]{c}, 1, 2}^{(l)} \NC \dots \NC Z_{\color[red]{c}, 1, \eta_x^{(l)}}^{(l)} \NR
\NC Z_{\color[red]{c}, 2, 1}^{(l)} \NC Z_{\color[red]{c}, 2, 2}^{(l)} \NC \dots \NC Z_{\color[red]{c}, 2, \eta_x^{(l)}}^{(l)} \NR
\NC \vdots \NC \vdots \NC \ddots \NC \vdots \NR
\NC Z_{\color[red]{c}, \eta_y^{(l)}, 1}^{(l)} \NC Z_{\color[red]{c}, \eta_y^{(l)}, 2}^{(l)} \NC \dots \NC Z_{\color[red]{c}, \eta_y^{(l)}, \eta_x^{(l)}}^{(l)} \NR
\stopmatrix
\stopformula

$W_{\color[blue]{c'}}^{l, \color[red]{c}}$ also represents a two dimensional matrix.
\startformula
W_{\color[blue]{c'}}^{l, \color[red]{c}} = 
\startmatrix[left={\left(}, right={\right)}]
\NC W_{\color[blue]{c'}, 1, 1}^{l, \color[red]{c}} \NC W_{\color[blue]{c'}, 1, 2}^{l, \color[red]{c}} \NC \dots \NC W_{\color[blue]{c'}, 1, \eta_x^{(l)}}^{l, \color[red]{c}} \NR
\NC W_{\color[blue]{c'}, 2, 1}^{l, \color[red]{c}} \NC W_{\color[blue]{c'}, 2, 2}^{l, \color[red]{c}} \NC \dots \NC W_{\color[blue]{c'}, 2, \eta_x^{(l)}}^{l, \color[red]{c}} \NR
\NC \vdots \NC \vdots \NC \ddots \NC \vdots \NR
\NC W_{\color[blue]{c'}, \eta_y^{(l)}, 1}^{l, \color[red]{c}} \NC W_{\color[blue]{c'}, \eta_y^{(l)}, 2}^{l, \color[red]{c}} \NC \dots \NC W_{\color[blue]{c'}, \eta_y^{(l)}, \eta_x^{(l)}}^{{l, \color[red]{c}}} \NR
\stopmatrix
\stopformula

I will denote the hadamard product by $\odot$.
Here we introduce the intermediary variable $H_{\color[blue]{c'}}^{(l,\color[red]{c})}$.
It is the hadamard product of $Z_{\color[red]{c}}^{(l)}$ with $W_{\color[blue]{c'}}^{l, \color[red]{c}}$.

\startformula
H_{\color[blue]{c'}}^{(l,\color[red]{c})} =
W_{\color[blue]{c'}}^{(l, \color[red]{c})}
\odot
Z_{\color[red]{c}}^{(l)} =
\sum_{\color[blue]{y'} = 0}^{\eta_y^{(l)}}
\sum_{\color[blue]{x'} = 0}^{\eta_x^{(l)}}
\Bigl(
W_{\color[blue]{c'}, \color[blue]{y'}, \color[blue]{x'}}^{(l, \color[red]{c})}
Z_{\color[blue]{c}, \color[red]{y} + \color[blue]{y'}, \color[red]{x} + \color[blue]{x'}}^{(l)}
\Bigr)
\stopformula

Substituting for it in Formula~\in[formula-conv-base] we get an expression early similar to Formula~\in[formula-mlp-base].

\startplaceformula
\startformula
Z^{(l+1)}_{\color[red]{c}} = 
\sum_{\color[blue]{c'} = 0}^{\eta_c^{(l)}}
\Bigl(
H_{\color[blue]{c'}}^{(l, \color[red]{c})} 
\Bigr) + 
b^{(l, \color[red]{c})}
\stopformula
\stopplaceformula

The hadamard product is nothing more than a long finite sum of different weights and products.
Taking the partial derivative of it--with respect ot any one of them---yields the cooresponding weight or product entry.
Thus we have that.

\startplaceformula
\startformula
\startmathalignment
\NC \frac
    {\partial H_{\color[blue]{c'}}^{(l,\color[red]{c})}}
    {\partial Z_{\color[red]{c}}^{(l)}} \NC =
W_{\color[blue]{c'}}^{(l, \color[red]{c})} \NR[+]
\NC \frac
    {\partial H_{\color[blue]{c'}}^{(l,\color[red]{c})}}
    {\partial W_{\color[blue]{c'}}^{(l, \color[red]{c})}} \NC =
Z_{\color[red]{c}}^{(l)} \NR[+]
\stopmathalignment
\stopformula
\stopplaceformula

If we scale up even further we get the formula which relates layer $l+1$ with layer $l$.

\startformula
H^{(l)} = 
\startmatrix[left={\left(}, right={\right)}]
\NC H^{(l, 1)} \NR
\NC H^{(l, 2)} \NR
\NC \vdots \NR
\NC H^{(l, \eta_c^{(l)})} \NR
\stopmatrix =
\startmatrix[left={\left(}, right={\right)}]
\NC \sum_{\color[blue]{c'} = 0}^{\eta_c^{(l)}}
\Bigl(
H_{\color[blue]{c'}}^{(l, 1)} 
\Bigr) \NR
\NC \sum_{\color[blue]{c'} = 0}^{\eta_c^{(l)}}
\Bigl(
H_{\color[blue]{c'}}^{(l, 2)} 
\Bigr) \NR
\NC \vdots \NR
\NC \sum_{\color[blue]{c'} = 0}^{\eta_c^{(l)}}
\Bigl(
H_{\color[blue]{c'}}^{(l, \eta_c^{(l)})} 
\Bigr) \NR
\stopmatrix
\stopformula


Let us now calculate the partial derivatives.

\startplaceformula
\startformula
\frac
    {\partial Z^{(l+1)}_{\color[red]{c}}}
    {Z^{(l)}_{\color[blue]{c'}}} =

\stopformula
\stopplaceformula

\stopsubsubsection
\stopsubsection

\startsubsection[title=Pooling layer]
A non thinking layer, good for adjusting the networks performance.
\stopsubsection

\startsubsection[title=Final layer]
\stopsubsection
\stopsection

\startformula
w_{(\color[blue]{c'}, \color[red]{c})}^{(l)}(y,x)
\stopformula